{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project - NLP-Driven Ingredient Health and Dietary Restriction Analysis\n",
    "\n",
    "*Name: Laura Obermaier*\n",
    "\n",
    "*Stevens ID: 20027358*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import spacy\n",
    "import pubchempy as pcp\n",
    "from rapidfuzz import process, fuzz\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "from spacy.lang.en import English\n",
    "import tiktoken\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment and Global Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laura\\anaconda3\\envs\\llama3gptq\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "\n",
    "csv.field_size_limit(2**20)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp_sentencizer = English()\n",
    "nlp_sentencizer.add_pipe(\"sentencizer\")\n",
    "\n",
    "global_alias_set = set()\n",
    "search_cache = {}\n",
    "alias_frequency = defaultdict(int)\n",
    "\n",
    "CACHE_FILE = \"search_cache.json\"\n",
    "ALIAS_FREQ_FILE = \"alias_frequency.json\"\n",
    "ALIAS_CACHE_FILE = \"alias_cache.json\"\n",
    "\n",
    "try:\n",
    "    with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        search_cache = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    search_cache = {}\n",
    "\n",
    "model_name_or_path = \"TechxGenus/Meta-Llama-3-8B-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    revision=\"main\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_search_cache():\n",
    "    try:\n",
    "        with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(search_cache, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to save search cache: {e}\")\n",
    "\n",
    "def save_alias_frequency(path=ALIAS_FREQ_FILE):\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(alias_frequency, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[✓] Alias frequency saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error saving alias frequency: {e}\")\n",
    "\n",
    "def load_alias_frequency(path=ALIAS_FREQ_FILE):\n",
    "    global alias_frequency\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            alias_frequency = defaultdict(int, {k: int(v) for k, v in data.items()})\n",
    "        print(f\"[✓] Loaded {len(alias_frequency)} alias frequencies from cache.\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ ] Alias frequency cache not found at {path}. Will regenerate.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error loading alias frequency: {e}\")\n",
    "        return False\n",
    "    \n",
    "def save_alias_cache(path=ALIAS_CACHE_FILE):\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(sorted(global_alias_set), f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[✓] Alias cache saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error saving alias cache: {e}\")\n",
    "\n",
    "def load_alias_cache(path=ALIAS_CACHE_FILE):\n",
    "    global global_alias_set\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            global_alias_set = set(json.load(f))\n",
    "        print(f\"[✓] Loaded {len(global_alias_set)} aliases from cache.\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ ] Alias cache not found at {path}. Will seed from source...\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error loading alias cache: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alias + Name Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_alias(alias):\n",
    "    alias_clean = alias.strip().lower()\n",
    "    if len(alias_clean.split()) > 4:\n",
    "        return False\n",
    "    if re.search(r'\\d{3,}|\\d+%|[^\\w\\s\\-]', alias_clean):  # long numeric sequence or symbols\n",
    "        if alias_frequency[alias_clean] < 5:  # require higher frequency to keep\n",
    "            return False\n",
    "    if re.search(r'^\\d{2,5}-\\d{2,5}-\\d$', alias_clean): \n",
    "        return False\n",
    "    if len(alias_clean) > 40:\n",
    "        return False\n",
    "    if alias_clean.count(',') > 0 or alias_clean.count('(') > 1:\n",
    "        return False\n",
    "    if any(keyword in alias_clean for keyword in ['acs', 'usp', 'grade', 'reference', 'powder', 'solution', 'mist']):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_pubchem_aliases(ingredient_name):\n",
    "    try:\n",
    "        compounds = pcp.get_compounds(ingredient_name, 'name')\n",
    "        if compounds:\n",
    "            synonyms = compounds[0].synonyms\n",
    "            filtered = [s.lower() for s in synonyms if is_relevant_alias(s)]\n",
    "            return list(set(filtered))\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"[PubChem error for '{ingredient_name}']: {e}\")\n",
    "        return []\n",
    "\n",
    "def update_alias_cache(aliases):\n",
    "    for a in aliases:\n",
    "        if a:\n",
    "            global_alias_set.add(a.lower().strip())\n",
    "\n",
    "def fuzzy_match_alias(name, threshold=90):\n",
    "    if not global_alias_set:\n",
    "        print(\"[Warning] Alias set is empty — did you run seed_aliases_from_open_food_facts?\")\n",
    "        return None\n",
    "    result = process.extractOne(name, global_alias_set, scorer=fuzz.token_sort_ratio)\n",
    "    if result is None:\n",
    "        return None\n",
    "    match, score, _ = result\n",
    "    return match if score >= threshold else None\n",
    "\n",
    "def is_valid_alias(alias, reference, threshold=85):\n",
    "    return fuzz.ratio(alias.lower(), reference.lower()) >= threshold\n",
    "\n",
    "def is_phonetically_valid(word):\n",
    "    word = word.lower()\n",
    "    if len(word) < 3:\n",
    "        return False\n",
    "    vowels = sum(1 for c in word if c in \"aeiou\")\n",
    "    consonants = sum(1 for c in word if c.isalpha() and c not in \"aeiou\")\n",
    "    if consonants == 0:\n",
    "        return False\n",
    "    ratio = vowels / (consonants + vowels)\n",
    "    return 0.2 <= ratio <= 0.8  # extremely low/high = junk\n",
    "\n",
    "def standardize_ingredient_name(name, max_aliases=5):\n",
    "    name = name.lower().strip()\n",
    "    aliases = get_pubchem_aliases(name)\n",
    "\n",
    "    if aliases:\n",
    "        update_alias_cache(aliases)\n",
    "        # Apply filters\n",
    "        filtered_aliases = [\n",
    "            a for a in aliases\n",
    "            if is_relevant_alias(a) and is_valid_alias(a, name)\n",
    "        ]\n",
    "\n",
    "        # Improved sort: prioritize exact match, then similarity, then length\n",
    "        ranked = sorted(\n",
    "            filtered_aliases,\n",
    "            key=lambda x: (\n",
    "                0 if x == name else 1,               # exact match first\n",
    "                -fuzz.token_sort_ratio(name, x),     # highest similarity\n",
    "                len(x)                               # shorter is better\n",
    "            )\n",
    "        )\n",
    "        top_aliases = ranked[:max_aliases]\n",
    "        if not top_aliases:\n",
    "            print(f\"[Alias Fallback] No valid aliases for '{name}', reverting to original.\")\n",
    "            return name, [name]\n",
    "        print(f\"[Query Alias] {name} → Filtered aliases: {top_aliases}\")\n",
    "        return top_aliases[0], top_aliases\n",
    "\n",
    "    fuzzy = fuzzy_match_alias(name)\n",
    "    if fuzzy:\n",
    "        print(f\"[Correction] '{name}' autocorrected to alias: '{fuzzy}'\")\n",
    "        return fuzzy, [fuzzy]\n",
    "\n",
    "    return name, [name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Food Facts Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Loaded 6095 aliases from cache.\n",
      "[✓] Loaded 6099 alias frequencies from cache.\n"
     ]
    }
   ],
   "source": [
    "def seed_aliases_from_open_food_facts(limit=10000):\n",
    "    url = \"https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.encoding = 'utf-8'\n",
    "\n",
    "    alias_dict = defaultdict(set)\n",
    "    lines = (line.decode('utf-8') for line in response.iter_lines())\n",
    "    reader = csv.DictReader(lines, delimiter='\\t')\n",
    "\n",
    "    langs = ['fr', 'de', 'es', 'it']\n",
    "    count = 0\n",
    "\n",
    "    for row in reader:\n",
    "        if count % 500 == 0:\n",
    "            print(f\"Processing row {count}...\")\n",
    "        if count >= limit:\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "        ingredients_text = row.get(\"ingredients_text\", \"\")\n",
    "        if not ingredients_text.strip():\n",
    "            continue\n",
    "\n",
    "        for ing in ingredients_text.split(','):\n",
    "            ing = ing.strip().lower()\n",
    "            if not ing:\n",
    "                continue\n",
    "            alias_dict[ing].add(ing)\n",
    "            alias_frequency[ing] += 1\n",
    "\n",
    "            for lang in langs:\n",
    "                key = f\"ingredients_text_{lang}\"\n",
    "                alt = row.get(key)\n",
    "                if alt:\n",
    "                    for alt_ing in alt.split(','):\n",
    "                        alt_ing = alt_ing.strip().lower()\n",
    "                        if alt_ing:\n",
    "                            alias_dict[ing].add(alt_ing)\n",
    "                            alias_dict[alt_ing].add(ing)\n",
    "\n",
    "    for aliases in alias_dict.values():\n",
    "        update_alias_cache(list(aliases))\n",
    "    print(f\"[✓] Seeded {len(global_alias_set)} unique aliases from Open Food Facts.\")\n",
    "\n",
    "loaded_alias = load_alias_cache()\n",
    "loaded_freqs = load_alias_frequency()\n",
    "\n",
    "if not (loaded_alias and loaded_freqs):\n",
    "    seed_aliases_from_open_food_facts(limit=5000)\n",
    "    save_alias_cache()\n",
    "    save_alias_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External API Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pubmed(ingredient, max_results=5):\n",
    "    try:\n",
    "        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "        params = {\"db\": \"pubmed\", \"term\": ingredient, \"retmode\": \"json\", \"retmax\": max_results}\n",
    "        ids = requests.get(base_url, params=params).json()[\"esearchresult\"].get(\"idlist\", [])\n",
    "        summaries = []\n",
    "        for pmid in ids:\n",
    "            summary_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "            r = requests.get(summary_url, params={\"db\": \"pubmed\", \"id\": pmid, \"retmode\": \"json\"}).json()\n",
    "            result = r[\"result\"].get(pmid)\n",
    "            if result:\n",
    "                summaries.append({\"title\": result.get(\"title\"), \"source\": result.get(\"source\"), \"pubdate\": result.get(\"pubdate\")})\n",
    "        return summaries\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def is_fda_entry_relevant(text, ingredient):\n",
    "    irrelevant_keywords = [\"recall\", \"undeclared\", \"labeling\", \"distribution\", \"pasteurization\", \"packaging\", \"incorrect\", \"contain\"]\n",
    "    text_lower = text.lower()\n",
    "    if any(kw in text_lower for kw in irrelevant_keywords):\n",
    "        return False\n",
    "    return ingredient.lower() in text_lower\n",
    "\n",
    "def query_openfda(ingredient):\n",
    "    try:\n",
    "        base_url = \"https://api.fda.gov/food/enforcement.json\"\n",
    "        params = {\"search\": f\"product_description:{ingredient}\", \"limit\": 5}\n",
    "        r = requests.get(base_url, params=params).json()\n",
    "        return [rec[\"reason_for_recall\"] for rec in r.get(\"results\", []) if is_fda_entry_relevant(rec[\"reason_for_recall\"], ingredient)]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def query_rxnorm(ingredient):\n",
    "    try:\n",
    "        url = \"https://rxnav.nlm.nih.gov/REST/rxcui.json\"\n",
    "        rxcui = requests.get(url, params={\"name\": ingredient}).json()\n",
    "        return rxcui.get(\"idGroup\", {}).get(\"rxnormId\", [])\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def query_academic_health_docs(ingredient, max_results=5):\n",
    "    def query_europe_pmc(max_results=5):\n",
    "        try:\n",
    "            base_url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "            query = f'\"{ingredient}\" AND (nutrition OR dietary OR health OR benefits OR concerns OR restrictions)' \\\n",
    "                    f' AND (\"nutrition science\" OR \"toxicology\" OR \"public health\") AND (\"human\")'\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"format\": \"json\",\n",
    "                \"resultType\": \"core\",\n",
    "                \"pageSize\": max_results\n",
    "            }\n",
    "            r = requests.get(base_url, params=params).json()\n",
    "            results = []\n",
    "            for record in r.get(\"resultList\", {}).get(\"result\", []):\n",
    "                abstract = record.get(\"abstractText\")\n",
    "                if not abstract or len(abstract.strip()) < 100:\n",
    "                    continue\n",
    "                results.append({\n",
    "                    \"title\": record.get(\"title\"),\n",
    "                    \"source\": record.get(\"journalTitle\"),\n",
    "                    \"pubdate\": record.get(\"firstPublicationDate\", record.get(\"pubYear\")),\n",
    "                    \"url\": f\"https://europepmc.org/article/{record.get('source')}/{record.get('id')}\",\n",
    "                    \"full_text\": abstract.strip()\n",
    "                })\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[EuropePMC JSON API error for '{ingredient}']: {e}\")\n",
    "            return []\n",
    "\n",
    "    def query_pubmed_central():\n",
    "        try:\n",
    "            search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "            params = {\n",
    "                \"db\": \"pmc\",\n",
    "                \"term\": f'\"{ingredient}\" AND (nutrition OR dietary OR health OR benefits)',\n",
    "                \"retmode\": \"json\",\n",
    "                \"retmax\": max_results,\n",
    "            }\n",
    "            r = requests.get(search_url, params=params).json()\n",
    "            ids = r.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "            summaries = []\n",
    "            for pmid in ids:\n",
    "                try:\n",
    "                    summary_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "                    summary_resp = requests.get(summary_url, params={\"db\": \"pmc\", \"id\": pmid, \"retmode\": \"json\"}).json()\n",
    "                    result = summary_resp.get(\"result\", {}).get(pmid)\n",
    "                    if not result:\n",
    "                        continue\n",
    "                    title = result.get(\"title\", \"\")\n",
    "                    url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmid}/\"\n",
    "                    summaries.append({\"title\": title, \"url\": url, \"full_text\": title})\n",
    "                except:\n",
    "                    continue\n",
    "            return summaries\n",
    "        except Exception as e:\n",
    "            print(f\"[PubMedCentral error for '{ingredient}']: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Combine and deduplicate by title\n",
    "    pmc_results = query_pubmed_central()\n",
    "    europepmc_results = query_europe_pmc()\n",
    "    combined = pmc_results + europepmc_results\n",
    "    seen_titles = set()\n",
    "    unique_results = []\n",
    "    for r in combined:\n",
    "        if r[\"title\"] and r[\"title\"] not in seen_titles:\n",
    "            unique_results.append(r)\n",
    "            seen_titles.add(r[\"title\"])\n",
    "    return unique_results[:max_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER + Semantic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text, aliases=None, health_keywords=None):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    invalid_labels = {\n",
    "        \"CARDINAL\", \"DATE\", \"ORDINAL\", \"PERCENT\", \"LANGUAGE\", \"TIME\", \"QUANTITY\", \"MONEY\", \"NORP\", \"EVENT\"\n",
    "    }\n",
    "    forbidden_words = {\n",
    "        \"recall\", \"product\", \"distribution\", \"ingredient\", \"label\", \"cookie\", \"brownie\", \"package\",\n",
    "        \"expiration\", \"sell\", \"pasteurization\"\n",
    "    }\n",
    "\n",
    "    spans = []\n",
    "    for ent in doc.ents:\n",
    "        span_text = ent.text.strip()\n",
    "        span_clean = span_text.lower()\n",
    "        label = ent.label_\n",
    "\n",
    "        # Re-tag known aliases incorrectly labeled as PERSON\n",
    "        if label == \"PERSON\" and aliases and span_clean in aliases:\n",
    "            print(f\"[NER Correction] '{span_text}' was labeled as PERSON, relabeling as INGREDIENT\")\n",
    "            label = \"INGREDIENT\"\n",
    "\n",
    "        # Skip irrelevant\n",
    "        if label in invalid_labels:\n",
    "            continue\n",
    "        if len(span_text) < 3:\n",
    "            continue\n",
    "        if any(word in span_clean for word in forbidden_words):\n",
    "            continue\n",
    "\n",
    "        # Contextual scoring for filtering or ranking\n",
    "        context_window = text[max(0, ent.start_char - 50):ent.end_char + 50].lower()\n",
    "        context_score = sum(1 for kw in (health_keywords or []) if kw in context_window)\n",
    "\n",
    "        spans.append((ent.start_char, ent.end_char, label, span_text, context_score))\n",
    "\n",
    "    # Deduplicate overlapping spans (keep most relevant)\n",
    "    merged = []\n",
    "    spans.sort(key=lambda x: (x[0], -(x[1]-x[0])))\n",
    "    for start, end, label, text_span, score in spans:\n",
    "        if merged and start < merged[-1][1]:\n",
    "            prev = merged[-1]\n",
    "            if score > prev[4]:  # keep if more relevant\n",
    "                merged[-1] = (start, end, label, text_span, score)\n",
    "        else:\n",
    "            merged.append((start, end, label, text_span, score))\n",
    "\n",
    "    return [(text[start:end], label) for start, end, label, _, _ in merged]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scraping and Google CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web_snippets(ingredient, num_results=5, api_key=None, cse_id=None):\n",
    "    if not api_key or not cse_id:\n",
    "        raise ValueError(\"Google API key and CSE ID are required.\")\n",
    "\n",
    "    if ingredient in search_cache:\n",
    "        return search_cache[ingredient]\n",
    "\n",
    "    query = f'\"{ingredient}\" AND (nutrition OR dietary OR health OR benefits OR concerns OR restrictions)'\n",
    "\n",
    "    try:\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "        res = service.cse().list(q=query, cx=cse_id, num=num_results).execute()\n",
    "        items = res.get(\"items\", [])\n",
    "        keywords = [\n",
    "            \"health\", \"benefit\", \"risk\", \"effect\", \"toxicity\", \"nutrition\", \"nutrient\", \"disease\", \"metabolism\", \"metabolic\", \n",
    "            \"wellness\", \"digestion\", \"digestive\", \"immunity\", \"immune\", \"safety\", \"intolerance\", \"allergy\", \"reaction\", \n",
    "            \"therapy\", \"treatment\", \"deficiency\", \"excess\", \"overdose\", \"lead to\", \"cause\", \"inflammation\", \"danger\", \"impact\", \n",
    "            \"cardiovascular\", \"result\", \"liver\", \"kidney\", \"blood\", \"hormone\", \"cholesterol\", \"diabetes\", \"body\", \"brain\", \n",
    "            \"mental\", \"physical\", \"absorption\", \"energy\", \"weight\", \"fat\", \"calories\", \"fitness\", \"detox\", \"toxins\", \n",
    "            \"cognitive\", \"focus\", \"clarity\", \"cancer\", \"symptom\", \"pain\", \"relief\", \"anti-inflammatory\", \"sensitivity\", \n",
    "            \"cramps\", \"gut\", \"probiotic\", \"enzyme\", \"alkaline\", \"acidic\", \"bloating\", \"constipation\", \"restriction\", \"avoid\"\n",
    "        ]\n",
    "        snippets = [item.get(\"snippet\", \"\") for item in items if any(k in item.get(\"snippet\", \"\").lower() for k in keywords)]\n",
    "        print(f\"[Google CSE Success] Retrieved {len(snippets)} snippets for '{ingredient}'\")\n",
    "        print(f\"[Google CSE Log] Query: {query}\")\n",
    "        for i, snippet in enumerate(snippets[:3]):\n",
    "            print(f\"  Snippet {i+1}: {snippet}\")\n",
    "        search_cache[ingredient] = snippets\n",
    "        return snippets\n",
    "    except Exception as e:\n",
    "        print(f\"[Google Search Error for '{ingredient}']: {e}\")\n",
    "        return []\n",
    "\n",
    "def semantic_scrape_summary(ingredient, api_key=None, cse_id=None):\n",
    "    fallback_attempts = 0\n",
    "    all_snippets = search_web_snippets(ingredient, api_key=api_key, cse_id=cse_id)\n",
    "\n",
    "    if not all_snippets:\n",
    "        fallback_attempts += 1\n",
    "        fallback_term = re.sub(r'[^\\w\\s]', '', ingredient)\n",
    "        print(f\"[Fallback] Trying sanitized alias: '{fallback_term}'\")\n",
    "        all_snippets = search_web_snippets(fallback_term, api_key=api_key, cse_id=cse_id)\n",
    "\n",
    "    if not all_snippets:\n",
    "        print(f\"[Fallback] Google CSE returned no snippets even after fallback. Skipping.\")\n",
    "        return []\n",
    "\n",
    "    all_ents = []\n",
    "    for i, snippet in enumerate(all_snippets):\n",
    "        if not snippet.strip():\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[Google Snippet #{i+1} for '{ingredient}']:\\n{snippet}\")\n",
    "        ents = extract_entities(snippet, aliases=global_alias_set, health_keywords=[\n",
    "            \"health\", \"benefit\", \"risk\", \"effect\", \"toxicity\", \"nutrition\", \"nutrient\", \"disease\", \"metabolism\", \"metabolic\",\n",
    "            \"wellness\", \"digestion\", \"digestive\", \"immunity\", \"immune\", \"safety\", \"intolerance\", \"allergy\", \"reaction\",\n",
    "            \"therapy\", \"treatment\", \"deficiency\", \"excess\", \"overdose\", \"lead to\", \"cause\", \"inflammation\", \"danger\", \"impact\",\n",
    "            \"cardiovascular\", \"result\", \"liver\", \"kidney\", \"blood\", \"hormone\", \"cholesterol\", \"diabetes\", \"body\", \"brain\",\n",
    "            \"mental\", \"physical\", \"absorption\", \"energy\", \"weight\", \"fat\", \"calories\", \"fitness\", \"detox\", \"toxins\",\n",
    "            \"cognitive\", \"focus\", \"clarity\", \"cancer\", \"symptom\", \"pain\", \"relief\", \"anti-inflammatory\", \"sensitivity\",\n",
    "            \"cramps\", \"gut\", \"probiotic\", \"enzyme\", \"alkaline\", \"acidic\", \"bloating\", \"constipation\", \"restriction\", \"avoid\"\n",
    "        ])\n",
    "        if ents:\n",
    "            print(\"  → Extracted Entities:\")\n",
    "            for text, label in ents:\n",
    "                print(f\"     - {text} ({label})\")\n",
    "        else:\n",
    "            print(\"  → No entities found.\")\n",
    "\n",
    "        all_ents.extend(ents)\n",
    "\n",
    "    return list(set(all_ents))  # Deduplicate final entity list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Processing Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_health_info(ingredient):\n",
    "    return {\n",
    "        \"PubMed\": query_pubmed(ingredient),\n",
    "        \"OpenFDA\": query_openfda(ingredient),\n",
    "        \"RxNorm\": query_rxnorm(ingredient),\n",
    "        \"Academic_Articles\": query_academic_health_docs(ingredient)\n",
    "    }\n",
    "\n",
    "def preprocess_ingredient_list_with_health(text):\n",
    "    raw_ingredients = re.split(r'[\\,\\n;/••]+', text)\n",
    "    processed = []\n",
    "    seen_terms = set()\n",
    "\n",
    "    for raw in raw_ingredients:\n",
    "        raw = raw.strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "\n",
    "        if not is_phonetically_valid(raw):\n",
    "            print(f\"[!] Skipping '{raw}' — unlikely to be a valid ingredient (too short or invalid vowel/consonant pattern)\")\n",
    "            continue\n",
    "\n",
    "        standard, aliases = standardize_ingredient_name(raw)\n",
    "        aliases = list(set([standard] + aliases))\n",
    "        filtered_aliases = [a for a in aliases if is_relevant_alias(a)]\n",
    "        \n",
    "        # Sort and limit to top 5 by similarity\n",
    "        ranked_aliases = sorted(\n",
    "            filtered_aliases,\n",
    "            key=lambda x: fuzz.token_sort_ratio(raw.lower(), x.lower()),\n",
    "            reverse=True\n",
    "        )\n",
    "        selected_aliases = ranked_aliases[:5]\n",
    "\n",
    "        combined_health_info = {\n",
    "            \"PubMed\": [],\n",
    "            \"OpenFDA\": [],\n",
    "            \"RxNorm\": [],\n",
    "            \"NER_Snippets\": [],\n",
    "            \"Academic_Articles\": []\n",
    "        }\n",
    "\n",
    "        found_any_data = False\n",
    "\n",
    "        for term in selected_aliases:\n",
    "            if term.lower() in seen_terms:\n",
    "                continue\n",
    "            seen_terms.add(term.lower())\n",
    "\n",
    "            api_info = get_all_health_info(term)\n",
    "            ner_info = semantic_scrape_summary(term, api_key=GOOGLE_API_KEY, cse_id=GOOGLE_CSE_ID)\n",
    "\n",
    "            if any(api_info[k] for k in api_info) or ner_info:\n",
    "                found_any_data = True\n",
    "\n",
    "            for k in combined_health_info:\n",
    "                if k == \"NER_Snippets\":\n",
    "                    combined_health_info[k].extend([i for i in ner_info if i not in combined_health_info[k]])\n",
    "                else:\n",
    "                    combined_health_info[k].extend([i for i in api_info[k] if i not in combined_health_info[k]])\n",
    "\n",
    "        if not found_any_data:\n",
    "            print(f\"⚠️ No data found for '{raw}'. Did you spell this correctly or is it too obscure?\")\n",
    "\n",
    "        processed.append({\n",
    "            \"standard\": standard,\n",
    "            \"aliases\": aliases,\n",
    "            \"health_info\": combined_health_info\n",
    "        })\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def split_into_token_chunks(text, max_tokens=2000, model=\"gpt-3.5-turbo\"):\n",
    "    doc = nlp_sentencizer(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sent in doc.sents:\n",
    "        if num_tokens(current_chunk + sent.text, model) > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent.text\n",
    "        else:\n",
    "            current_chunk += \" \" + sent.text\n",
    "\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens_per_message = 4  # each message key structure (role, content, etc.)\n",
    "    tokens = 0\n",
    "    for message in messages:\n",
    "        tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            tokens += len(encoding.encode(value))\n",
    "    return tokens + 2  # every reply is primed with <|start|>assistant\n",
    "\n",
    "def classify_chunk(chunk, model=\"your-local-llm\"):\n",
    "    # Placeholder: use your own model here\n",
    "    # This could be a call to a HuggingFace pipeline or custom model\n",
    "    return \"Category: [Benefits]\\nReason: Simulated classification for now.\"\n",
    "\n",
    "def summarize_by_category(classified_chunks, model=\"your-local-llm\"):\n",
    "    # Placeholder: simulate summary for now\n",
    "    summaries = {}\n",
    "    for cat, texts in defaultdict(list).items():\n",
    "        summaries[cat] = \"Simulated summary.\"\n",
    "    return summaries\n",
    "\n",
    "def enrich_with_health_summaries(results, model=\"gpt-3.5-turbo\"):\n",
    "    for entry in results:\n",
    "        all_chunks = []\n",
    "\n",
    "        for article in entry[\"health_info\"].get(\"Academic_Articles\", []):\n",
    "            text = article.get(\"full_text\")\n",
    "            if text and len(text.strip()) > 200:\n",
    "                chunks = split_into_token_chunks(text, max_tokens=2000, model=model)\n",
    "                all_chunks.extend(chunks)\n",
    "\n",
    "        if not all_chunks:\n",
    "            # Try fallback: summarize from available titles + sources\n",
    "            fallback_chunks = []\n",
    "            for article in entry[\"health_info\"].get(\"Academic_Articles\", []):\n",
    "                title = article.get(\"title\", \"\").strip()\n",
    "                source = article.get(\"source\", \"\").strip()\n",
    "                if len(title) > 20:\n",
    "                    snippet = f\"Title: {title}\"\n",
    "                    if source:\n",
    "                        snippet += f\" | Source: {source}\"\n",
    "                    fallback_chunks.append(snippet)\n",
    "\n",
    "            if fallback_chunks:\n",
    "                classified = [(chunk, classify_chunk(chunk, model=model)) for chunk in fallback_chunks]\n",
    "                summaries = summarize_by_category(classified, model=model)\n",
    "                entry[\"health_summary\"] = summaries\n",
    "            else:\n",
    "                entry[\"health_summary\"] = {\"note\": \"No usable full-text content or fallback title.\"}\n",
    "            continue\n",
    "\n",
    "        # Regular summarization path\n",
    "        classified = [(chunk, classify_chunk(chunk, model=model)) for chunk in all_chunks]\n",
    "        summaries = summarize_by_category(classified, model=model)\n",
    "        entry[\"health_summary\"] = summaries\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### llama3gptq integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(ingredient_name, facts_chunk, task=\"benefits\"):\n",
    "    if task == \"benefits\":\n",
    "        instruction = f\"Given these health-related findings about {ingredient_name}, summarize ONLY the beneficial effects for human health in at most 150 words. Ignore negative or neutral findings.\"\n",
    "    elif task == \"concerns\":\n",
    "        instruction = f\"Given these health-related findings about {ingredient_name}, summarize ONLY the negative health concerns, risks, or dangers in at most 150 words. Ignore benefits or unrelated findings.\"\n",
    "    elif task == \"restrictions\":\n",
    "        instruction = f\"Given these health-related findings about {ingredient_name}, summarize ONLY the dietary restrictions or populations that should limit intake, in at most 150 words. Ignore benefits.\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task type: {task}\")\n",
    "\n",
    "    return instruction + \"\\n\\n\" + facts_chunk\n",
    "\n",
    "def query_llama(prompt, max_new_tokens=150):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# --- New Summarization Functions ---\n",
    "\n",
    "def generate_summary(text, category, ingredient_name, model, tokenizer):\n",
    "    if category == \"benefits\":\n",
    "        instruction = f\"Given these health-related findings about {ingredient_name}, summarize ONLY the beneficial effects for human health in at most 150 words. Ignore negative or neutral findings.\"\n",
    "    elif category == \"concerns\":\n",
    "        instruction = f\"Given these health-related findings about {ingredient_name}, summarize ONLY the negative health effects, risks, or concerns in at most 150 words. Ignore benefits or unrelated information.\"\n",
    "    elif category == \"restrictions\":\n",
    "        instruction = f\"Given these health-related findings about {ingredient_name}, summarize any known dietary restrictions, intolerances, or allergies in at most 150 words. Ignore unrelated or general health information.\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid category\")\n",
    "\n",
    "    full_prompt = instruction + \"\\n\\n\" + text\n",
    "\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=150, pad_token_id=tokenizer.eos_token_id)\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary.strip()\n",
    "\n",
    "\n",
    "def clean_and_chunk_sentences(text, health_keywords=None, max_sentences=20):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if health_keywords:\n",
    "        sentences = [s for s in sentences if any(kw in s.lower() for kw in health_keywords)]\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), max_sentences):\n",
    "        chunk = \" \".join(sentences[i:i+max_sentences])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- Update enrichment function ---\n",
    "\n",
    "def enrich_with_health_summaries_v2(results, model, tokenizer):\n",
    "    health_keywords = [\n",
    "        \"health\", \"benefit\", \"risk\", \"effect\", \"toxicity\", \"nutrition\", \"disease\", \"immunity\", \n",
    "        \"safety\", \"reaction\", \"therapy\", \"treatment\", \"deficiency\", \"cause\", \"danger\", \n",
    "        \"inflammation\", \"cardiovascular\", \"liver\", \"kidney\", \"cholesterol\", \"diabetes\", \n",
    "        \"absorption\", \"weight\", \"calories\", \"detox\", \"toxin\", \"cancer\", \"pain\", \"allergy\", \n",
    "        \"intolerance\", \"restriction\", \"avoid\", \"bloating\", \"digestion\"\n",
    "    ]\n",
    "\n",
    "    for entry in results:\n",
    "        ingredient_name = entry['standard']\n",
    "        articles = entry[\"health_info\"].get(\"Academic_Articles\", [])\n",
    "\n",
    "        if not articles:\n",
    "            entry[\"health_summary\"] = {\"note\": \"No academic articles available.\"}\n",
    "            continue\n",
    "\n",
    "        combined_text = \" \".join([a.get(\"full_text\", \"\") for a in articles if a.get(\"full_text\")])\n",
    "        if not combined_text.strip():\n",
    "            entry[\"health_summary\"] = {\"note\": \"No usable text.\"}\n",
    "            continue\n",
    "\n",
    "        chunks = clean_and_chunk_sentences(combined_text, health_keywords=health_keywords, max_sentences=20)\n",
    "        combined_for_summary = \" \".join(chunks)\n",
    "\n",
    "        # Generate category-wise summaries\n",
    "        entry[\"health_summary\"] = {\n",
    "            \"benefits\": generate_summary(combined_for_summary, \"benefits\", ingredient_name, model, tokenizer),\n",
    "            \"concerns\": generate_summary(combined_for_summary, \"concerns\", ingredient_name, model, tokenizer),\n",
    "            \"restrictions\": generate_summary(combined_for_summary, \"restrictions\", ingredient_name, model, tokenizer)\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Query Alias] sodium chloride → Filtered aliases: ['sodium chloride', 'sodium-36 chloride', 'sodium chloric', 'sodium monochloride', 'sodium chloride salt']\n",
      "\n",
      "[Google Snippet #1 for 'sodium chloride']:\n",
      "What are the side effects of sodium chloride? For the most part, sodium chloride isn't a health hazard, but in excessive amounts, it can irritate your ...\n",
      "  → No entities found.\n",
      "\n",
      "[Google Snippet #2 for 'sodium chloride']:\n",
      "Dietary Reference Intakes for Water, Potassium, Sodium, Chloride, and Sulfate The Dietary Reference Intakes (DRIs) are quantitative estimates of nutrient ...\n",
      "  → Extracted Entities:\n",
      "     - Sodium (ORG)\n",
      "     - Chloride (ORG)\n",
      "     - Sulfate The Dietary Reference Intakes (WORK_OF_ART)\n",
      "\n",
      "[Google Snippet #3 for 'sodium chloride']:\n",
      "Mar 16, 2024 ... Talk to a healthcare provider about the amount of sodium chloride that you can safely consume. Factors such as your age, chronic health problems ...\n",
      "  → Extracted Entities:\n",
      "     - Mar 16, 2024 (PERSON)\n",
      "\n",
      "[Google Snippet #4 for 'sodium chloride']:\n",
      "Dietary sodium chloride intake independently predicts the degree of hyperchloremic metabolic acidosis in healthy humans consuming a net acid-producing diet.\n",
      "  → No entities found.\n",
      "\n",
      "[Google Snippet #1 for 'sodium-36 chloride']:\n",
      "Sodium-36 chloride · Athomer · Cleanoz · Isotonic · Licefreee · Nebusal · Pulmosal ... U.S. Department of Health, Education Welfare, Public Health Service. Center ...\n",
      "[NER Correction] 'Cleanoz' was labeled as PERSON, relabeling as INGREDIENT\n",
      "[NER Correction] 'Nebusal' was labeled as PERSON, relabeling as INGREDIENT\n",
      "[NER Correction] 'Pulmosal' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Cleanoz (INGREDIENT)\n",
      "     - Nebusal (INGREDIENT)\n",
      "     - Pulmosal (INGREDIENT)\n",
      "     - U.S. Department of Health, Education Welfare, Public Health Service (ORG)\n",
      "\n",
      "[Google Snippet #2 for 'sodium-36 chloride']:\n",
      "Sodium Chloride Health & Safety Information ... Halite, Sodium Monochloride, Common salt, Rock salt, Table salt, CAS 32343-72-9 (Na36Cl, Sodium-36 chloride) ...\n",
      "[NER Correction] 'Sodium Monochloride' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Sodium Chloride Health & Safety Information (ORG)\n",
      "     - Sodium Monochloride (INGREDIENT)\n",
      "     - Common (PRODUCT)\n",
      "     - CAS (ORG)\n",
      "     - Na36Cl (ORG)\n",
      "\n",
      "[Google Snippet #1 for 'sodium chloric']:\n",
      "... Sodium chloric, Sodium chloride (NaCl), Hyposaline, Flexivial, Gingivyl ... Industries. Rubber · Lubricants · Agriculture · Animal Nutrition · Food & Nutrition ...\n",
      "[NER Correction] 'NaCl' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Sodium (ORG)\n",
      "     - NaCl (INGREDIENT)\n",
      "     - Hyposaline (ORG)\n",
      "     - Flexivial (GPE)\n",
      "     - Nutrition · Food & Nutrition (ORG)\n",
      "\n",
      "[Google Snippet #2 for 'sodium chloric']:\n",
      "Health Effects - (M)SDS · First Aid. Close. Sorry! ECHA has not published ... Agrosan; Atlacide; Chlorate de sodium; Chlorate salt of sodium; Chloric acid ...\n",
      "  → Extracted Entities:\n",
      "     - First Aid (ORG)\n",
      "     - ECHA (PERSON)\n",
      "     - Agrosan (PERSON)\n",
      "     - Chlorate (ORG)\n",
      "     - Chloric (ORG)\n",
      "\n",
      "[Google Snippet #3 for 'sodium chloric']:\n",
      "... Sodium chloric, Sodium chloride (NaCl), Hyposaline, Flexivial, Gingivyl ... Food & Nutrition · Lubricants · Energy Services. Get in touch. We'll look into ...\n",
      "[NER Correction] 'NaCl' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Sodium (ORG)\n",
      "     - NaCl (INGREDIENT)\n",
      "     - Hyposaline (ORG)\n",
      "     - Flexivial (GPE)\n",
      "     - Food & Nutrition (ORG)\n",
      "     - Energy Services (ORG)\n",
      "\n",
      "[Google Snippet #4 for 'sodium chloric']:\n",
      "Jul 14, 2015 ... HMIS Health Ratings (0-4):. Health: 1. Flammability: 0. Physical: 0 ... Halite, Rock salt, Table salt, Saline, Common salt, Sodium chloric.\n",
      "[NER Correction] 'Halite' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - HMIS Health Ratings (ORG)\n",
      "     - Halite (INGREDIENT)\n",
      "     - Saline (ORG)\n",
      "     - Sodium (ORG)\n",
      "\n",
      "[Google Snippet #5 for 'sodium chloric']:\n",
      "Rock salt · Dendritis · Purex · Sodium chloric · Sodium chloride (NaCl) · Flexivial ... health regulations. Hazardous Substances Data Bank (HSDB). Product: Offer ...\n",
      "[NER Correction] 'Dendritis' was labeled as PERSON, relabeling as INGREDIENT\n",
      "[NER Correction] 'NaCl' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Dendritis (INGREDIENT)\n",
      "     - NaCl (INGREDIENT)\n",
      "     - Hazardous Substances Data Bank (ORG)\n",
      "\n",
      "[Google Snippet #1 for 'sodium monochloride']:\n",
      "Find sodium monochloride and related products for scientific research at MilliporeSigma. ... Health Data Privacy Policy. An unknown error has occured ...\n",
      "  → Extracted Entities:\n",
      "     - MilliporeSigma (ORG)\n",
      "     - Health Data Privacy Policy (ORG)\n",
      "\n",
      "[Google Snippet #2 for 'sodium monochloride']:\n",
      "Rock salt · Dendritis · Purex · Sodium chloric · Sodium chloride (NaCl) · Flexivial · Gingivyl · Hyposaline · Sodium monochloride ... U.S. Department of Health, ...\n",
      "[NER Correction] 'Dendritis' was labeled as PERSON, relabeling as INGREDIENT\n",
      "[NER Correction] 'NaCl' was labeled as PERSON, relabeling as INGREDIENT\n",
      "[NER Correction] 'Flexivial' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Dendritis (INGREDIENT)\n",
      "     - NaCl (INGREDIENT)\n",
      "     - Flexivial (INGREDIENT)\n",
      "     - U.S. Department of Health (ORG)\n",
      "\n",
      "[Google Snippet #3 for 'sodium monochloride']:\n",
      "Natrium Chloride, Sodium monochloride, Halite, NaCl. Sign Into View ... Health Data Privacy Policy. English - EN; Español - ES. Learn More. 2025-04-15T03:30 ...\n",
      "[NER Correction] 'Halite' was labeled as PERSON, relabeling as INGREDIENT\n",
      "[NER Correction] 'NaCl' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Natrium Chloride (ORG)\n",
      "     - Sodium (PRODUCT)\n",
      "     - Halite (INGREDIENT)\n",
      "     - NaCl (INGREDIENT)\n",
      "     - Español - ES (ORG)\n",
      "\n",
      "[Google Snippet #4 for 'sodium monochloride']:\n",
      "May 6, 2015 ... Other Designations: Sodium monochloride; table salt; sea salt; common salt ... Potential Health Effects (Acute, Chronic and Delayed):.\n",
      "  → Extracted Entities:\n",
      "     - Potential Health Effects (Acute (ORG)\n",
      "     - Delayed (PERSON)\n",
      "\n",
      "[Google Snippet #1 for 'sodium chloride salt']:\n",
      "U.S. Department of Health, Education Welfare, Public Health Service. ... sodium chloride. Salt is a requirement in the diet. The safe and adequate ...\n",
      "  → Extracted Entities:\n",
      "     - U.S. Department of Health, Education Welfare, Public Health Service (ORG)\n",
      "\n",
      "[Google Snippet #2 for 'sodium chloride salt']:\n",
      "Oct 3, 2022 ... What are the health effects of home softening? A water softener that uses sodium chloride (salt) increases the amount of sodium in the water ...\n",
      "  → No entities found.\n",
      "\n",
      "[Google Snippet #3 for 'sodium chloride salt']:\n",
      "Apr 11, 2019 ... ... sodium chloride (salt). (More information); The National Academy of ... Effects on blood pressure of reduced dietary sodium and the Dietary ...\n",
      "[NER Correction] 'sodium chloride' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - sodium chloride (INGREDIENT)\n",
      "     - The National Academy of ... Effects (ORG)\n",
      "\n",
      "[Google Snippet #4 for 'sodium chloride salt']:\n",
      "Jan 31, 2024 ... Nutrition | Family Healthy Weight Programs | About Healthy Weight and Growth ... Salt is also known by its chemical name, sodium chloride. Salt is ...\n",
      "[NER Correction] 'Salt' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - | Family Healthy Weight Programs (PERSON)\n",
      "     - Healthy Weight (PERSON)\n",
      "     - Salt (INGREDIENT)\n",
      "[Query Alias] ascorbic acid → Filtered aliases: ['ascorbic acid', 'l-ascorbic acid', 'ascorbicum acidum']\n",
      "\n",
      "[Google Snippet #1 for 'ascorbic acid']:\n",
      "Learn about Vitamin C (Ascorbic acid) or find a doctor at Mount Sinai Health System ... benefit of other antioxidants and nutrients contained in food. If ...\n",
      "[NER Correction] 'Vitamin C' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Vitamin C (INGREDIENT)\n",
      "     - Mount Sinai Health System (ORG)\n",
      "\n",
      "[Google Snippet #2 for 'ascorbic acid']:\n",
      "Vitamin C (ascorbic acid) is a nutrient your body needs to form blood ... health tips, current health topics, and expertise on managing health. Click ...\n",
      "[NER Correction] 'Vitamin C' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Vitamin C (INGREDIENT)\n",
      "\n",
      "[Google Snippet #3 for 'ascorbic acid']:\n",
      "Vitamin C, also known as ascorbic acid, has several important functions. These include: helping to protect cells and keeping them healthy; maintaining ...\n",
      "[NER Correction] 'Vitamin C' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Vitamin C (INGREDIENT)\n",
      "\n",
      "[Google Snippet #1 for 'l-ascorbic acid']:\n",
      "L-ascorbic acid is an essential part of the human diet and has been associated ... L-ascorbic acid and health outcomes. Publication types. Meta-Analysis ...\n",
      "  → Extracted Entities:\n",
      "     - Meta-Analysis (ORG)\n",
      "\n",
      "[Google Snippet #2 for 'l-ascorbic acid']:\n",
      "L-ascorbic acid has been proven to provide powerful antioxidant benefits that help protect the skin from environmental damage while improving the visible signs ...\n",
      "  → No entities found.\n",
      "\n",
      "[Google Snippet #3 for 'l-ascorbic acid']:\n",
      "... L-ascorbic acid, which has allowed pharmacological levels of ascorbic ... 16. Levine MConry-Cantilena CWang Y et al. Vitamin C pharmacokinetics in healthy ...\n",
      "  → Extracted Entities:\n",
      "     - Levine MConry-Cantilena (PERSON)\n",
      "\n",
      "[Google Snippet #1 for 'ascorbicum acidum']:\n",
      "... Ascorbicum Acidum. 135. Ascorbic Acid, L-. 136. Cetebe. 137. Ascorbin. 138. (+) ... Dietary Reference Intakes: The Essential Guide to Nutrient Requirements ...\n",
      "[NER Correction] 'Ascorbicum Acidum' was labeled as PERSON, relabeling as INGREDIENT\n",
      "[NER Correction] 'Ascorbin' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Ascorbicum Acidum (INGREDIENT)\n",
      "     - Ascorbin (INGREDIENT)\n",
      "     - The Essential Guide to Nutrient Requirements (ORG)\n",
      "\n",
      "[Google Snippet #2 for 'ascorbicum acidum']:\n",
      "Agnus Castus 6X, Aralia Quinquefolia 4X, Ascorbicum Acidum 6X, Caladium ... 1-10 drops under the tongue, 3 times a day or as directed by a health professional.\n",
      "[NER Correction] 'Ascorbicum Acidum' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Aralia Quinquefolia 4X (PERSON)\n",
      "     - Ascorbicum Acidum (INGREDIENT)\n",
      "     - Caladium (GPE)\n",
      "\n",
      "[Google Snippet #3 for 'ascorbicum acidum']:\n",
      "Ascorbicum Acidum (Vitamin C) 6x · Salix Alba 4x 6x · Vitamin A 6x ... Professional Health Products®, PHP® and MethylGenetic Nutrition® are registered trademarks ...\n",
      "[NER Correction] 'Ascorbicum Acidum' was labeled as PERSON, relabeling as INGREDIENT\n",
      "[NER Correction] 'Vitamin C' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Ascorbicum Acidum (INGREDIENT)\n",
      "     - Vitamin C (INGREDIENT)\n",
      "     - Salix Alba (PERSON)\n",
      "     - PHP® (ORG)\n",
      "     - MethylGenetic Nutrition® (ORG)\n",
      "\n",
      "[Google Snippet #4 for 'ascorbicum acidum']:\n",
      "... Ascorbicum Acidum 6X, Boron 12X, Calcarea Phosphorica 12X, Calcium Gluconate ... If pregnant or breastfeeding, ask a health professional before use.\n",
      "[NER Correction] 'Ascorbicum Acidum' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Ascorbicum Acidum (INGREDIENT)\n",
      "     - Boron (PERSON)\n",
      "     - Calcarea Phosphorica 12X (PERSON)\n",
      "     - Calcium Gluconate (ORG)\n",
      "\n",
      "[Google Snippet #5 for 'ascorbicum acidum']:\n",
      "HHS Logo U.S. Department of Health & Human Services Divider Arrow NIH ... ASCORBICUM ACIDUM. Common Name, English. ASCORBATE. Common Name, English. Showing ...\n",
      "  → Extracted Entities:\n",
      "     - HHS Logo U.S. Department of Health & Human Services (ORG)\n",
      "     - Arrow NIH (PERSON)\n",
      "     - Common Name (PERSON)\n",
      "     - Common Name (PERSON)\n",
      "\n",
      "[Google Snippet #1 for 'curcuma longa']:\n",
      "Sep 2, 2017 ... Curcumin, an active component of turmeric (Curcuma longa), and its effects on health ... a Faculty of Health Sciences, Nutrition and Dietetics ...\n",
      "  → Extracted Entities:\n",
      "     - Curcumin (PERSON)\n",
      "     - Faculty of Health Sciences, (ORG)\n",
      "\n",
      "[Google Snippet #2 for 'curcuma longa']:\n",
      "Oct 1, 2013 ... Effect of dietary Curcuma longa on intestinal lesions following Eimeria maxima infection. One-day-old broiler chickens were fed a standard diet ...\n",
      "  → Extracted Entities:\n",
      "     - Eimeria (PERSON)\n",
      "[!] Skipping 'E300' — unlikely to be a valid ingredient (too short or invalid vowel/consonant pattern)\n",
      "[Correction] 'sal' autocorrected to alias: 'sal'\n",
      "\n",
      "[Google Snippet #1 for 'sal']:\n",
      "Learn more about Sal D'Allura, DO, a physician at Atrium Health Wake Forest Baptist providing world-class care across the region.\n",
      "  → Extracted Entities:\n",
      "     - Sal D'Allura (PERSON)\n",
      "     - Atrium Health Wake Forest Baptist (ORG)\n",
      "\n",
      "[Google Snippet #2 for 'sal']:\n",
      "Skip to main content. Fat Sal's Homepage · Locations; Menus. Current Menu · Catering Menu · Merchandise · About · Blog · Order Online · Fat Sal's Homepage.\n",
      "  → Extracted Entities:\n",
      "     - Fat Sal's (PERSON)\n",
      "     - Menus (GPE)\n",
      "     - Fat Sal's (PERSON)\n",
      "\n",
      "[Google Snippet #3 for 'sal']:\n",
      "Nov 30, 2021 ... Judy and Sal Troia made another generous donation to the DFMCH Integrative Health Program establishing the Judy L. and Sal A. Troia ...\n",
      "  → Extracted Entities:\n",
      "     - Judy (PERSON)\n",
      "     - Sal Troia (PERSON)\n",
      "     - the DFMCH Integrative Health Program (ORG)\n",
      "     - Judy L. (PERSON)\n",
      "     - Sal A. Troia (PERSON)\n",
      "\n",
      "[Google Snippet #4 for 'sal']:\n",
      "Mehtab Sal, MD Not currently accepting patients Diagnostic Radiology Resident About me Insurance Oregon Health & Science University is dedicated to improving ...\n",
      "  → Extracted Entities:\n",
      "     - Mehtab Sal (PERSON)\n",
      "     - MD Not (ORG)\n",
      "     - Diagnostic Radiology Resident (ORG)\n",
      "     - Insurance Oregon Health & Science University (ORG)\n",
      "\n",
      "[Google Snippet #1 for 'suagr']:\n",
      "Apr 11, 2025 ... Suagr Level Of 80 Creates Low Blood Sugar Symptoms How To Treat High Blood Sugar - Geoportal Oficial Del Instituto Geográfico Nacional De ...\n",
      "  → Extracted Entities:\n",
      "     - Suagr Level (PERSON)\n",
      "\n",
      "[Google Snippet #2 for 'suagr']:\n",
      "Sorbitol Healthy Sugar Low Calories Suagr CAS 50-70-4, Find Details about Sorbitol, Sorbitol Powder from Sorbitol Healthy Sugar Low Calories Suagr CAS ...\n",
      "[NER Correction] 'Sorbitol' was labeled as PERSON, relabeling as INGREDIENT\n",
      "  → Extracted Entities:\n",
      "     - Sorbitol Healthy Sugar (PERSON)\n",
      "     - Sorbitol (INGREDIENT)\n",
      "     - Sorbitol Powder (PERSON)\n",
      "     - Sorbitol Healthy Sugar (PERSON)\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\laura/nltk_data'\n    - 'c:\\\\Users\\\\laura\\\\anaconda3\\\\envs\\\\llama3gptq\\\\nltk_data'\n    - 'c:\\\\Users\\\\laura\\\\anaconda3\\\\envs\\\\llama3gptq\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\laura\\\\anaconda3\\\\envs\\\\llama3gptq\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\laura\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m results4 \u001b[38;5;241m=\u001b[39m enrich_with_health_summaries(results4)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Run LLM-based summarization on your enriched ingredient data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m results4 \u001b[38;5;241m=\u001b[39m \u001b[43menrich_with_health_summaries_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 74\u001b[0m, in \u001b[0;36menrich_with_health_summaries_v2\u001b[1;34m(results, model, tokenizer)\u001b[0m\n\u001b[0;32m     71\u001b[0m     entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhealth_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnote\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo usable text.\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mclean_and_chunk_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhealth_keywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhealth_keywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_sentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m combined_for_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Generate category-wise summaries\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 39\u001b[0m, in \u001b[0;36mclean_and_chunk_sentences\u001b[1;34m(text, health_keywords, max_sentences)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclean_and_chunk_sentences\u001b[39m(text, health_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_sentences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m---> 39\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m health_keywords:\n\u001b[0;32m     41\u001b[0m         sentences \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(kw \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m health_keywords)]\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\llama3gptq\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\llama3gptq\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\llama3gptq\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\llama3gptq\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\envs\\llama3gptq\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\laura/nltk_data'\n    - 'c:\\\\Users\\\\laura\\\\anaconda3\\\\envs\\\\llama3gptq\\\\nltk_data'\n    - 'c:\\\\Users\\\\laura\\\\anaconda3\\\\envs\\\\llama3gptq\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\laura\\\\anaconda3\\\\envs\\\\llama3gptq\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\laura\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sample4 = \"sodium chloride, ascorbic acid, curcuma longa, E300, sal, suagr\"\n",
    "results4 = preprocess_ingredient_list_with_health(sample4)\n",
    "results4 = enrich_with_health_summaries(results4)\n",
    "\n",
    "# Run LLM-based summarization on your enriched ingredient data\n",
    "results4 = enrich_with_health_summaries_v2(results4, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results cleanly\n",
    "for entry in results4:\n",
    "    print(f\"\\n🧪 Ingredient: {entry['standard']}\")\n",
    "    print(\"→ Aliases:\", entry[\"aliases\"])\n",
    "\n",
    "    print(\"\\nTrusted API Info:\")\n",
    "    for source, data in entry[\"health_info\"].items():\n",
    "        if source == \"NER_Snippets\":\n",
    "            continue\n",
    "        print(f\"  • {source}:\")\n",
    "        if isinstance(data, list) and data:\n",
    "            for item in data:\n",
    "                if isinstance(item, str):\n",
    "                    print(f\"     - {item}\")\n",
    "                elif isinstance(item, dict):\n",
    "                    print(f\"     - {item.get('title', '')}\")\n",
    "        elif isinstance(data, list):\n",
    "            print(\"     - No results\")\n",
    "        else:\n",
    "            print(f\"     - {data}\")\n",
    "\n",
    "    print(\"\\nNER Entities from Web Snippets:\")\n",
    "    if entry[\"health_info\"][\"NER_Snippets\"]:\n",
    "        for ent_text, ent_label in entry[\"health_info\"][\"NER_Snippets\"]:\n",
    "            print(f\"     - {ent_text} ({ent_label})\")\n",
    "    else:\n",
    "        print(\"     - No named entities found.\")\n",
    "\n",
    "    print(\"\\n💬 LLM-Generated Health Summaries:\")\n",
    "    for category, summary in entry.get(\"health_summary\", {}).items():\n",
    "        print(f\"  [{category.upper()}]: {summary}\")\n",
    "\n",
    "save_search_cache()\n",
    "save_alias_cache()\n",
    "save_alias_frequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"TechxGenus/Meta-Llama-3-8B-GPTQ\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "# Set manual memory limits (fit everything on GPU)\n",
    "max_memory = {0: \"23GiB\", \"cpu\": \"30GiB\"}  # 32GB RAM minus OS usage\n",
    "\n",
    "# Load model manually (no device_map=\"auto\" anymore)\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    device_map={\"\": 0},   # force full model on GPU 0\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "# Prepare prompt\n",
    "prompt = \"Explain the health benefits of vitamin C.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADD SENTIMENT EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPLEMENT LLM FOR AN OVERALL SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADD EVALUATION METRICS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
