{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project - NLP-Driven Ingredient Health and Dietary Restriction Analysis\n",
    "\n",
    "*Name: Laura Obermaier*\n",
    "\n",
    "*Stevens ID: 20027358*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredient List Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[1;32m---> 22\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m csv\u001b[38;5;241m.\u001b[39mfield_size_limit(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load English tokenizer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\laura\\anaconda3\\Lib\\site-packages\\openai\\_client.py:114\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    112\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import spacy\n",
    "import random\n",
    "import pubchempy as pcp\n",
    "from rapidfuzz import process, fuzz\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import io\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from duckduckgo_search import DDGS\n",
    "from bs4 import BeautifulSoup\n",
    "from duckduckgo_search.exceptions import DuckDuckGoSearchException\n",
    "from difflib import SequenceMatcher\n",
    "from spacy.lang.en import English\n",
    "from openai import OpenAI\n",
    "from collections import defaultdict\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "csv.field_size_limit(2**20)\n",
    "# Load English tokenizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Global cache to track discovered aliases\n",
    "global_alias_set = set()\n",
    "\n",
    "# Local search result cache\n",
    "search_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### alias relivancy filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_alias(alias):\n",
    "    alias_clean = alias.strip().lower()\n",
    "    if len(alias_clean.split()) > 4:\n",
    "        return False\n",
    "    if re.search(r'\\d{3,}|\\d+%|[^\\w\\s\\-]', alias_clean):\n",
    "        return False\n",
    "    if re.search(r'^\\d{2,5}-\\d{2,5}-\\d$', alias_clean): \n",
    "        return False\n",
    "    if len(alias_clean) > 40:\n",
    "        return False\n",
    "    if alias_clean.count(',') > 0 or alias_clean.count('(') > 1:\n",
    "        return False\n",
    "    if any(keyword in alias_clean for keyword in ['acs', 'usp', 'grade', 'reference', 'powder', 'solution', 'mist']):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PubChem(aliases) integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pubchem_aliases(ingredient_name):\n",
    "    try:\n",
    "        compounds = pcp.get_compounds(ingredient_name, 'name')\n",
    "        if compounds:\n",
    "            synonyms = compounds[0].synonyms\n",
    "            filtered = [s.lower() for s in synonyms if is_relevant_alias(s)]\n",
    "            return list(set(filtered))\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"[PubChem error for '{ingredient_name}']: {e}\")\n",
    "        return []\n",
    "\n",
    "def update_alias_cache(aliases):\n",
    "    for a in aliases:\n",
    "        if a:\n",
    "            global_alias_set.add(a.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fuzzy matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match_alias(name, threshold=90):\n",
    "    if not global_alias_set:\n",
    "        print(\"[Warning] Alias set is empty — did you run seed_aliases_from_open_food_facts?\")\n",
    "        return None\n",
    "    result = process.extractOne(name, global_alias_set, scorer=fuzz.token_sort_ratio)\n",
    "    if result is None:\n",
    "        return None\n",
    "    match, score, _ = result\n",
    "    return match if score >= threshold else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### seed aliases from Open Food Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_aliases_from_open_food_facts(limit=10000):\n",
    "    url = \"https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.encoding = 'utf-8'\n",
    "\n",
    "    alias_dict = defaultdict(set)\n",
    "    lines = (line.decode('utf-8') for line in response.iter_lines())\n",
    "    reader = csv.DictReader(lines, delimiter='\\t')\n",
    "\n",
    "    langs = ['fr', 'de', 'es', 'it']\n",
    "    count = 0\n",
    "\n",
    "    for row in reader:\n",
    "        if count % 500 == 0:\n",
    "            print(f\"Processing row {count}...\")\n",
    "        if count >= limit:\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "        ingredients_text = row.get(\"ingredients_text\", \"\")\n",
    "        if not ingredients_text.strip():\n",
    "            continue\n",
    "\n",
    "        for ing in ingredients_text.split(','):\n",
    "            ing = ing.strip().lower()\n",
    "            if not ing:\n",
    "                continue\n",
    "            alias_dict[ing].add(ing)\n",
    "\n",
    "            for lang in langs:\n",
    "                key = f\"ingredients_text_{lang}\"\n",
    "                alt = row.get(key)\n",
    "                if alt:\n",
    "                    for alt_ing in alt.split(','):\n",
    "                        alt_ing = alt_ing.strip().lower()\n",
    "                        if alt_ing:\n",
    "                            alias_dict[ing].add(alt_ing)\n",
    "                            alias_dict[alt_ing].add(ing)\n",
    "\n",
    "    for aliases in alias_dict.values():\n",
    "        update_alias_cache(list(aliases))\n",
    "    print(f\"[✓] Seeded {len(global_alias_set)} unique aliases from Open Food Facts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_ingredient_name(name, max_aliases=5):\n",
    "    name = name.lower().strip()\n",
    "    aliases = get_pubchem_aliases(name)\n",
    "    \n",
    "    # Filter and rank aliases if any were found\n",
    "    if aliases:\n",
    "        update_alias_cache(aliases)\n",
    "        # Remove irrelevant aliases\n",
    "        filtered_aliases = [a for a in aliases if is_relevant_alias(a)]\n",
    "        # Rank aliases by similarity to input name\n",
    "        ranked = sorted(filtered_aliases, key=lambda x: fuzz.token_sort_ratio(name, x), reverse=True)\n",
    "        top_aliases = ranked[:max_aliases]\n",
    "        return top_aliases[0] if top_aliases else name, top_aliases or [name]\n",
    "\n",
    "    fuzzy = fuzzy_match_alias(name)\n",
    "    if fuzzy:\n",
    "        return fuzzy, [fuzzy]\n",
    "\n",
    "    return name, [name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Web-based Health info retrieval via trusted APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pubmed(ingredient, max_results=5):\n",
    "    try:\n",
    "        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "        params = {\"db\": \"pubmed\", \"term\": ingredient, \"retmode\": \"json\", \"retmax\": max_results}\n",
    "        ids = requests.get(base_url, params=params).json()[\"esearchresult\"].get(\"idlist\", [])\n",
    "        summaries = []\n",
    "        for pmid in ids:\n",
    "            summary_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "            r = requests.get(summary_url, params={\"db\": \"pubmed\", \"id\": pmid, \"retmode\": \"json\"}).json()\n",
    "            result = r[\"result\"].get(pmid)\n",
    "            if result:\n",
    "                summaries.append({\"title\": result.get(\"title\"), \"source\": result.get(\"source\"), \"pubdate\": result.get(\"pubdate\")})\n",
    "        return summaries\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def query_openfda(ingredient):\n",
    "    try:\n",
    "        base_url = \"https://api.fda.gov/food/enforcement.json\"\n",
    "        params = {\"search\": f\"product_description:{ingredient}\", \"limit\": 5}\n",
    "        r = requests.get(base_url, params=params).json()\n",
    "        return [rec[\"reason_for_recall\"] for rec in r.get(\"results\", [])]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def query_rxnorm(ingredient):\n",
    "    try:\n",
    "        url = \"https://rxnav.nlm.nih.gov/REST/rxcui.json\"\n",
    "        rxcui = requests.get(url, params={\"name\": ingredient}).json()\n",
    "        return rxcui.get(\"idGroup\", {}).get(\"rxnormId\", [])\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query Academic Health Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_academic_health_docs(ingredient, max_results=5):\n",
    "    def query_pubmed_central():\n",
    "        try:\n",
    "            search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "            params = {\n",
    "                \"db\": \"pmc\",\n",
    "                \"term\": f'\"{ingredient}\" \\\n",
    "                    AND (\"well-being\" OR \"wellbeing\" OR \"diet\" OR \"dietary\" OR \"nutrition\" OR \"health\" OR \"benefits\" OR \"concerns\" OR \"dietary restrictions\") \\\n",
    "                    AND (\"nutrition science\" OR \"toxicology\" OR \"dietary science\" OR \"public health\") \\\n",
    "                    AND (\"human\" OR \"people\" OR \"men\" OR \"women\" OR \"individual\" OR \"clinical study\") \\\n",
    "                    AND (\"systematic review\" OR \"meta-analysis\")',\n",
    "                \"retmode\": \"json\",\n",
    "                \"retmax\": max_results,\n",
    "            }\n",
    "            r = requests.get(search_url, params=params).json()\n",
    "            ids = r.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "            summaries = []\n",
    "            for pmid in ids:\n",
    "                # Summary metadata\n",
    "                summary_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "                summary_resp = requests.get(summary_url, params={\"db\": \"pmc\", \"id\": pmid, \"retmode\": \"json\"}).json()\n",
    "                result = summary_resp[\"result\"].get(pmid)\n",
    "\n",
    "                # Full text via efetch\n",
    "                fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "                fetch_resp = requests.get(fetch_url, params={\n",
    "                    \"db\": \"pmc\",\n",
    "                    \"id\": pmid,\n",
    "                    \"retmode\": \"xml\"\n",
    "                })\n",
    "                full_text = fetch_resp.text if fetch_resp.status_code == 200 else None\n",
    "\n",
    "                if result:\n",
    "                    summaries.append({\n",
    "                        \"title\": result.get(\"title\"),\n",
    "                        \"source\": result.get(\"source\"),\n",
    "                        \"pubdate\": result.get(\"pubdate\"),\n",
    "                        \"url\": f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmid}/\",\n",
    "                        \"full_text\": full_text\n",
    "                    })\n",
    "            return summaries\n",
    "        except Exception as e:\n",
    "            print(f\"[PubMedCentral error for '{ingredient}']: {e}\")\n",
    "            return []\n",
    "\n",
    "    def query_europe_pmc():\n",
    "        try:\n",
    "            base_url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "            query = f'\"{ingredient}\" \\\n",
    "                AND (\"well-being\" OR \"wellbeing\" OR \"diet\" OR \"dietary\" OR \"nutrition\" OR \"health\" OR \"benefits\" OR \"concerns\" OR \"dietary restrictions\") \\\n",
    "                AND (\"nutrition science\" OR \"toxicology\" OR \"dietary science\" OR \"public health\") \\\n",
    "                AND (\"human\" OR \"people\" OR \"men\" OR \"women\" OR \"individual\" OR \"clinical study\") \\\n",
    "                AND (\"systematic review\" OR \"meta-analysis\")'\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"format\": \"json\",\n",
    "                \"resultType\": \"core\",\n",
    "                \"pageSize\": max_results\n",
    "            }\n",
    "            r = requests.get(base_url, params=params).json()\n",
    "            results = []\n",
    "            for record in r.get(\"resultList\", {}).get(\"result\", []):\n",
    "                url = None\n",
    "                full_text = None\n",
    "                # Attempt to extract the full-text URL\n",
    "                for ft in record.get(\"fullTextUrlList\", {}).get(\"fullTextUrl\", []):\n",
    "                    if ft.get(\"documentStyle\") == \"html\" and ft.get(\"url\"):\n",
    "                        url = ft[\"url\"]\n",
    "                        try:\n",
    "                            page = requests.get(url, timeout=10)\n",
    "                            if page.status_code == 200:\n",
    "                                full_text = BeautifulSoup(page.text, \"html.parser\").get_text()\n",
    "                        except Exception as e:\n",
    "                            print(f\"[EuropePMC full text fetch failed]: {e}\")\n",
    "                        break\n",
    "                results.append({\n",
    "                    \"title\": record.get(\"title\"),\n",
    "                    \"source\": record.get(\"journalTitle\"),\n",
    "                    \"pubdate\": record.get(\"firstPublicationDate\", record.get(\"pubYear\")),\n",
    "                    \"url\": url,\n",
    "                    \"full_text\": full_text\n",
    "                })\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[EuropePMC error for '{ingredient}']: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Combine and deduplicate by title\n",
    "    pmc_results = query_pubmed_central()\n",
    "    europepmc_results = query_europe_pmc()\n",
    "    combined = pmc_results + europepmc_results\n",
    "    seen_titles = set()\n",
    "    unique_results = []\n",
    "    for r in combined:\n",
    "        if r[\"title\"] and r[\"title\"] not in seen_titles:\n",
    "            unique_results.append(r)\n",
    "            seen_titles.add(r[\"title\"])\n",
    "    return unique_results[:max_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER + Semantic Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NER + Semantic Web Search ---\n",
    "ner_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "def search_web_snippets(ingredient, num_results=5):\n",
    "    if ingredient in search_cache:\n",
    "        return search_cache[ingredient]\n",
    "    queries = [f'\"{ingredient}\" AND (\"well-being\" OR \"wellbeing\" OR \"diet\" OR \"dietary\" OR \"nutrition\" OR \"health\" OR \"benefits\" OR \"concerns\" OR \"dietary restrictions\") AND (\"nutrition science\" OR \"toxicology\" OR \"dietary science\" OR \"public health\") AND (\"human\" OR \"people\" OR \"men\" OR \"women\" OR \"individual\" OR \"clinical study\") AND (\"systematic review\" OR \"meta-analysis\") AND (language:English) AND (publication_date:[2005-01-01 TO 2025-01-01]) NOT (\"synthetic route\" OR \"polymer\" OR \"conference abstract\")']\n",
    "    snippets = []\n",
    "\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "            for q in queries:\n",
    "                retries = 0\n",
    "                while retries < 3:\n",
    "                    try:\n",
    "                        results = ddgs.text(q, max_results=num_results)\n",
    "                        for res in results:\n",
    "                            if 'body' in res:\n",
    "                                snippets.append(res['body'])\n",
    "                            if 'title' in res:\n",
    "                                snippets.append(res['title'])\n",
    "                        break  # success\n",
    "                    except Exception as e:\n",
    "                        retries += 1\n",
    "                        wait = 2 ** retries + random.uniform(0, 1)\n",
    "                        print(f\"[Retry {retries}] DuckDuckGo error for '{ingredient}': {e} — waiting {wait:.2f}s\")\n",
    "                        time.sleep(wait)\n",
    "\n",
    "        search_cache[ingredient] = snippets\n",
    "        return snippets\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[DuckDuckGo Error for '{ingredient}']: {e}\")\n",
    "        return []\n",
    "\n",
    "def semantic_scrape_summary(ingredient):\n",
    "    snippets = search_web_snippets(ingredient)\n",
    "    all_ents = []\n",
    "    for text in snippets:\n",
    "        if not text or len(text.strip()) < 20:\n",
    "            continue\n",
    "        ents = extract_entities(text)\n",
    "        all_ents.extend(ents)\n",
    "    return all_ents\n",
    "\n",
    "def get_all_health_info(ingredient):\n",
    "    return {\n",
    "        \"PubMed\": query_pubmed(ingredient),\n",
    "        \"OpenFDA\": query_openfda(ingredient),\n",
    "        \"RxNorm\": query_rxnorm(ingredient),\n",
    "        \"Academic_Articles\": query_academic_health_docs(ingredient)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ingredient list processing based on health info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ingredient_list_with_health(text):\n",
    "    raw_ingredients = re.split(r'[\\,\\n;/••]+', text)\n",
    "    processed = []\n",
    "    seen_terms = set()\n",
    "\n",
    "    for raw in raw_ingredients:\n",
    "        raw = raw.strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "\n",
    "        standard, aliases = standardize_ingredient_name(raw)\n",
    "\n",
    "        filtered_aliases = [a for a in list(set([standard] + aliases)) if is_relevant_alias(a)]\n",
    "        combined_health_info = {\"PubMed\": [], \"OpenFDA\": [], \"RxNorm\": [], \"NER_Snippets\": [], \"Academic_Articles\": []}\n",
    "\n",
    "        for term in filtered_aliases:\n",
    "            if term.lower() in seen_terms:\n",
    "                continue\n",
    "            seen_terms.add(term.lower())\n",
    "\n",
    "            api_info = get_all_health_info(term)\n",
    "            ner_info = semantic_scrape_summary(term)\n",
    "\n",
    "            for k in combined_health_info:\n",
    "                if k == \"NER_Snippets\":\n",
    "                    combined_health_info[k].extend([i for i in ner_info if i not in combined_health_info[k]])\n",
    "                else:\n",
    "                    combined_health_info[k].extend([i for i in api_info[k] if i not in combined_health_info[k]])\n",
    "\n",
    "        processed.append({\n",
    "            \"standard\": standard,\n",
    "            \"aliases\": aliases,\n",
    "            \"health_info\": combined_health_info\n",
    "        })\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### store aliases for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIAS_CACHE_FILE = \"alias_cache.json\"\n",
    "\n",
    "def save_alias_cache(path=ALIAS_CACHE_FILE):\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(sorted(global_alias_set), f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[✓] Alias cache saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error saving alias cache: {e}\")\n",
    "\n",
    "def load_alias_cache(path=ALIAS_CACHE_FILE):\n",
    "    global global_alias_set\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            global_alias_set = set(json.load(f))\n",
    "        print(f\"[✓] Loaded {len(global_alias_set)} aliases from cache.\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ ] Alias cache not found at {path}. Will seed from source...\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error loading alias cache: {e}\")\n",
    "        return False\n",
    "\n",
    "if not load_alias_cache():\n",
    "    seed_aliases_from_open_food_facts(limit=5000)\n",
    "    save_alias_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence splitter for document chunking\n",
    "nlp_sentencizer = English()\n",
    "nlp_sentencizer.add_pipe(\"sentencizer\")\n",
    "\n",
    "def split_into_chunks(text, max_words=200):\n",
    "    doc = nlp_sentencizer(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_len = 0\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        tokens = sent.text.strip().split()\n",
    "        if current_len + len(tokens) > max_words:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_len = 0\n",
    "        current_chunk.append(sent.text.strip())\n",
    "        current_len += len(tokens)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_chunk(chunk, model=\"gpt-3.5-turbo\"):\n",
    "    prompt = f\"\"\"\n",
    "You are a medical and dietary research assistant. Given the paragraph below, classify it into one or more of the following:\n",
    "- Benefits\n",
    "- Concerns\n",
    "- Dietary Restrictions\n",
    "\n",
    "Respond in this format:\n",
    "Category: [Category 1, Category 2, ...]\n",
    "Reason: [Short explanation]\n",
    "\n",
    "Paragraph:\n",
    "\\\"\\\"\\\"\n",
    "{chunk}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarization by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_by_category(classified_chunks, model=\"gpt-3.5-turbo\"):\n",
    "    grouped = defaultdict(list)\n",
    "    for chunk, result in classified_chunks:\n",
    "        match = re.search(r\"Category:\\s*\\[(.*?)\\]\", result)\n",
    "        if match:\n",
    "            cats = [c.strip() for c in match.group(1).split(\",\")]\n",
    "            for cat in cats:\n",
    "                grouped[cat].append(chunk)\n",
    "\n",
    "    summaries = {}\n",
    "    for cat, texts in grouped.items():\n",
    "        joined = \"\\n\".join(texts[:10])  # Limit per category for performance\n",
    "        prompt = f\"Summarize the following text into key findings related to {cat.lower()}:\\n\\n{joined}\"\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample4 = \"sodium chloride, ascorbic acid, curcuma longa, E300, sal, suagr\"\n",
    "results4 = preprocess_ingredient_list_with_health(sample4)\n",
    "\n",
    "for entry in results4:\n",
    "    all_chunks = []\n",
    "    for article in entry[\"health_info\"].get(\"Academic_Articles\", []):\n",
    "        text = article.get(\"full_text\")\n",
    "        if text and len(text.strip()) > 200:\n",
    "            chunks = split_into_chunks(text)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "    # Skip empty ones\n",
    "    if not all_chunks:\n",
    "        entry[\"health_summary\"] = {\"note\": \"No usable full-text content\"}\n",
    "        continue\n",
    "\n",
    "    classified = [(chunk, classify_chunk(chunk)) for chunk in all_chunks]\n",
    "    summaries = summarize_by_category(classified)\n",
    "    entry[\"health_summary\"] = summaries\n",
    "\n",
    "for entry in results4:\n",
    "    print(\"\\nIngredient:\", entry[\"standard\"])\n",
    "    print(\"→ Aliases:\", entry[\"aliases\"])\n",
    "\n",
    "    print(\"\\nTrusted API Info:\")\n",
    "    for source, data in entry[\"health_info\"].items():\n",
    "        if source == \"NER_Snippets\":\n",
    "            continue\n",
    "        print(f\"  • {source}:\")\n",
    "        if isinstance(data, list) and data:\n",
    "            for item in data:\n",
    "                print(f\"     - {item}\")\n",
    "        elif isinstance(data, list):\n",
    "            print(\"     - No results\")\n",
    "        else:\n",
    "            print(f\"     - {data}\")\n",
    "\n",
    "    print(\"\\nNER Entities from Web Snippets:\")\n",
    "    if entry[\"health_info\"][\"NER_Snippets\"]:\n",
    "        for ent_text, ent_label in entry[\"health_info\"][\"NER_Snippets\"]:\n",
    "            print(f\"     - {ent_text} ({ent_label})\")\n",
    "    else:\n",
    "        print(\"     - No named entities found.\")\n",
    "\n",
    "    print(\"\\nSummarized Health Insights:\")\n",
    "    if isinstance(entry.get(\"health_summary\"), dict):\n",
    "        for cat, summary in entry[\"health_summary\"].items():\n",
    "            print(f\"  [{cat}]: {summary}\")\n",
    "    else:\n",
    "        print(\"  - No summary generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
