{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project - NLP-Driven Ingredient Health and Dietary Restriction Analysis\n",
    "\n",
    "*Name: Laura Obermaier*\n",
    "\n",
    "*Stevens ID: 20027358*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import requests\n",
    "import spacy\n",
    "import pubchempy as pcp\n",
    "from rapidfuzz import process, fuzz\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "from spacy.lang.en import English\n",
    "import tiktoken\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "import torch\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment and Global Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CSE_ID = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "\n",
    "csv.field_size_limit(2**20)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp_sentencizer = English()\n",
    "nlp_sentencizer.add_pipe(\"sentencizer\")\n",
    "\n",
    "global_alias_set = set()\n",
    "search_cache = {}\n",
    "alias_frequency = defaultdict(int)\n",
    "\n",
    "CACHE_FILE = \"search_cache.json\"\n",
    "ALIAS_FREQ_FILE = \"alias_frequency.json\"\n",
    "ALIAS_CACHE_FILE = \"alias_cache.json\"\n",
    "\n",
    "try:\n",
    "    with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        search_cache = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    search_cache = {}\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    use_safetensors=True,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "keyword_config = {\n",
    "    \"benefits\": [\n",
    "        \"benefit\", \"supports\", \"improves\", \"boosts\", \"enhances\", \"aids\", \"reduces\", \"prevents\",\n",
    "        \"protects\", \"promotes\", \"stimulates\", \"strengthens\", \"aiding\", \"improving\", \"healing\",\n",
    "        \"facilitates\", \"enhancing\", \"balances\", \"restores\", \"ameliorates\", \"treats\", \"alleviates\",\n",
    "        \"relieves\", \"contributes to\", \"beneficial\", \"positive\", \"advantageous\", \"favorable\",\n",
    "        \"healthy\", \"wellness\", \"nutrient\", \"nutritional\", \"immune\", \"well-being\", \"absorption\",\n",
    "        \"energy\", \"fitness\", \"cognitive\", \"focus\", \"clarity\", \"relief\", \"anti-inflammatory\",\n",
    "        \"cramps\", \"gut\", \"probiotic\", \"enzyme\", \"alkaline\", \"acidic\", \"bloating\", \"constipation\"\n",
    "    ],\n",
    "    \"concerns\": [\n",
    "        \"risk\", \"toxic\", \"harm\", \"adverse\", \"cause\", \"increased\", \"linked to\", \"danger\",\n",
    "        \"poisonous\", \"unsafe\", \"negatively\", \"exacerbates\", \"side effect\",\n",
    "        \"carcinogenic\", \"neurotoxic\", \"hepatotoxic\", \"irritation\", \"may lead to\", \"trigger\",\n",
    "        \"overdoes\", \"reaction\", \"symptom\", \"pain\", \"toxins\"\n",
    "    ],\n",
    "    \"restrictions\": [\n",
    "        \"allergy\", \"intolerance\", \"sensitivity\", \"restricted\", \"avoid\", \"not suitable\",\n",
    "        \"contraindicated\", \"dietary restriction\", \"religious restriction\", \"vegan\", \"vegetarian\",\n",
    "        \"halal\", \"haram\", \"gluten\", \"lactose\", \"kosher\", \"FODMAP\", \"contains\", \"may contain\",\n",
    "        \"cross-contamination\", \"not recommended\", \"not advised\", \"not suitable for\",\n",
    "        \"not safe for\"\n",
    "    ],\n",
    "    \"neutral\": [\n",
    "        \"kidney\", \"liver\", \"cholesterol\", \"diabetes\", \"inflammation\", \"cancer\", \"cardiovascular\",\n",
    "        \"digestion\", \"therapy\", \"treatment\", \"metabolism\", \"metabolic\", \"calories\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Compose superset for health relevance filtering\n",
    "keyword_config[\"all\"] = list(set(\n",
    "    keyword_config[\"benefits\"] +\n",
    "    keyword_config[\"concerns\"] +\n",
    "    keyword_config[\"restrictions\"] +\n",
    "    keyword_config[\"neutral\"] +\n",
    "    [\"health\", \"nutrition\", \"disease\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_search_cache():\n",
    "    try:\n",
    "        with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(search_cache, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to save search cache: {e}\")\n",
    "\n",
    "def save_alias_frequency(path=ALIAS_FREQ_FILE):\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(alias_frequency, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[✓] Alias frequency saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error saving alias frequency: {e}\")\n",
    "\n",
    "def load_alias_frequency(path=ALIAS_FREQ_FILE):\n",
    "    global alias_frequency\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            alias_frequency = defaultdict(int, {k: int(v) for k, v in data.items()})\n",
    "        print(f\"[✓] Loaded {len(alias_frequency)} alias frequencies from cache.\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ ] Alias frequency cache not found at {path}. Will regenerate.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error loading alias frequency: {e}\")\n",
    "        return False\n",
    "    \n",
    "def save_alias_cache(path=ALIAS_CACHE_FILE):\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(sorted(global_alias_set), f, ensure_ascii=False, indent=2)\n",
    "        print(f\"[✓] Alias cache saved to {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error saving alias cache: {e}\")\n",
    "\n",
    "def load_alias_cache(path=ALIAS_CACHE_FILE):\n",
    "    global global_alias_set\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            global_alias_set = set(json.load(f))\n",
    "        print(f\"[✓] Loaded {len(global_alias_set)} aliases from cache.\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ ] Alias cache not found at {path}. Will seed from source...\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Error loading alias cache: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alias + Name Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant_alias(alias):\n",
    "    alias_clean = alias.strip().lower()\n",
    "    if len(alias_clean.split()) > 4:\n",
    "        return False\n",
    "    if re.search(r'\\d{3,}|\\d+%|[^\\w\\s\\-]', alias_clean):  # long numeric sequence or symbols\n",
    "        if alias_frequency[alias_clean] < 5:  # require higher frequency to keep\n",
    "            return False\n",
    "    if re.search(r'^\\d{2,5}-\\d{2,5}-\\d$', alias_clean): \n",
    "        return False\n",
    "    if len(alias_clean) > 40:\n",
    "        return False\n",
    "    if alias_clean.count(',') > 0 or alias_clean.count('(') > 1:\n",
    "        return False\n",
    "    if any(keyword in alias_clean for keyword in ['acs', 'usp', 'grade', 'reference', 'powder', 'solution', 'mist']):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_pubchem_aliases(ingredient_name):\n",
    "    try:\n",
    "        compounds = pcp.get_compounds(ingredient_name, 'name')\n",
    "        if compounds:\n",
    "            synonyms = compounds[0].synonyms\n",
    "            filtered = [s.lower() for s in synonyms if is_relevant_alias(s)]\n",
    "            return list(set(filtered))\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"[PubChem error for '{ingredient_name}']: {e}\")\n",
    "        return []\n",
    "\n",
    "def update_alias_cache(aliases):\n",
    "    for a in aliases:\n",
    "        if a:\n",
    "            global_alias_set.add(a.lower().strip())\n",
    "\n",
    "def fuzzy_match_alias(name, threshold=90):\n",
    "    if not global_alias_set:\n",
    "        print(\"[Warning] Alias set is empty — did you run seed_aliases_from_open_food_facts?\")\n",
    "        return None\n",
    "    result = process.extractOne(name, global_alias_set, scorer=fuzz.token_sort_ratio)\n",
    "    if result is None:\n",
    "        return None\n",
    "    match, score, _ = result\n",
    "    return match if score >= threshold else None\n",
    "\n",
    "def is_valid_alias(alias, reference, threshold=85):\n",
    "    return fuzz.ratio(alias.lower(), reference.lower()) >= threshold\n",
    "\n",
    "def is_phonetically_valid(word):\n",
    "    word = word.lower()\n",
    "    if len(word) < 3:\n",
    "        return False\n",
    "    vowels = sum(1 for c in word if c in \"aeiou\")\n",
    "    consonants = sum(1 for c in word if c.isalpha() and c not in \"aeiou\")\n",
    "    if consonants == 0:\n",
    "        return False\n",
    "    ratio = vowels / (consonants + vowels)\n",
    "    return 0.2 <= ratio <= 0.8  # extremely low/high = junk\n",
    "\n",
    "def standardize_ingredient_name(name, max_aliases=5):\n",
    "    name = name.lower().strip()\n",
    "    aliases = get_pubchem_aliases(name)\n",
    "\n",
    "    if aliases:\n",
    "        update_alias_cache(aliases)\n",
    "        # Apply filters\n",
    "        filtered_aliases = [\n",
    "            a for a in aliases\n",
    "            if is_relevant_alias(a) and is_valid_alias(a, name)\n",
    "        ]\n",
    "\n",
    "        # Improved sort: prioritize exact match, then similarity, then length\n",
    "        ranked = sorted(\n",
    "            filtered_aliases,\n",
    "            key=lambda x: (\n",
    "                0 if x == name else 1,               # exact match first\n",
    "                -fuzz.token_sort_ratio(name, x),     # highest similarity\n",
    "                len(x)                               # shorter is better\n",
    "            )\n",
    "        )\n",
    "        top_aliases = ranked[:max_aliases]\n",
    "        if not top_aliases:\n",
    "            print(f\"[Alias Fallback] No valid aliases for '{name}', reverting to original.\")\n",
    "            return name, [name]\n",
    "        print(f\"[Query Alias] {name} → Filtered aliases: {top_aliases}\")\n",
    "        return top_aliases[0], top_aliases\n",
    "\n",
    "    fuzzy = fuzzy_match_alias(name)\n",
    "    if fuzzy:\n",
    "        print(f\"[Correction] '{name}' autocorrected to alias: '{fuzzy}'\")\n",
    "        return fuzzy, [fuzzy]\n",
    "\n",
    "    return name, [name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Food Facts Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_aliases_from_open_food_facts(limit=10000):\n",
    "    url = \"https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.encoding = 'utf-8'\n",
    "\n",
    "    alias_dict = defaultdict(set)\n",
    "    lines = (line.decode('utf-8') for line in response.iter_lines())\n",
    "    reader = csv.DictReader(lines, delimiter='\\t')\n",
    "\n",
    "    langs = ['fr', 'de', 'es', 'it']\n",
    "    count = 0\n",
    "\n",
    "    for row in reader:\n",
    "        if count % 500 == 0:\n",
    "            print(f\"Processing row {count}...\")\n",
    "        if count >= limit:\n",
    "            break\n",
    "        count += 1\n",
    "\n",
    "        ingredients_text = row.get(\"ingredients_text\", \"\")\n",
    "        if not ingredients_text.strip():\n",
    "            continue\n",
    "\n",
    "        for ing in ingredients_text.split(','):\n",
    "            ing = ing.strip().lower()\n",
    "            if not ing:\n",
    "                continue\n",
    "            alias_dict[ing].add(ing)\n",
    "            alias_frequency[ing] += 1\n",
    "\n",
    "            for lang in langs:\n",
    "                key = f\"ingredients_text_{lang}\"\n",
    "                alt = row.get(key)\n",
    "                if alt:\n",
    "                    for alt_ing in alt.split(','):\n",
    "                        alt_ing = alt_ing.strip().lower()\n",
    "                        if alt_ing:\n",
    "                            alias_dict[ing].add(alt_ing)\n",
    "                            alias_dict[alt_ing].add(ing)\n",
    "\n",
    "    for aliases in alias_dict.values():\n",
    "        update_alias_cache(list(aliases))\n",
    "    print(f\"[✓] Seeded {len(global_alias_set)} unique aliases from Open Food Facts.\")\n",
    "\n",
    "loaded_alias = load_alias_cache()\n",
    "loaded_freqs = load_alias_frequency()\n",
    "\n",
    "if not (loaded_alias and loaded_freqs):\n",
    "    seed_aliases_from_open_food_facts(limit=5000)\n",
    "    save_alias_cache()\n",
    "    save_alias_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External API Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pubmed(ingredient, max_results=50):\n",
    "    try:\n",
    "        base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "        params = {\"db\": \"pubmed\", \"term\": ingredient, \"retmode\": \"json\", \"retmax\": max_results}\n",
    "        ids = requests.get(base_url, params=params).json()[\"esearchresult\"].get(\"idlist\", [])\n",
    "        summaries = []\n",
    "        for pmid in ids:\n",
    "            summary_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "            r = requests.get(summary_url, params={\"db\": \"pubmed\", \"id\": pmid, \"retmode\": \"json\"}).json()\n",
    "            result = r[\"result\"].get(pmid)\n",
    "            if result:\n",
    "                summaries.append({\"title\": result.get(\"title\"), \"source\": result.get(\"source\"), \"pubdate\": result.get(\"pubdate\")})\n",
    "        return summaries\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def is_fda_entry_relevant(text, ingredient):\n",
    "    irrelevant_keywords = [\"recall\", \"undeclared\", \"labeling\", \"distribution\", \"pasteurization\", \"packaging\", \"incorrect\", \"contain\"]\n",
    "    text_lower = text.lower()\n",
    "    if any(kw in text_lower for kw in irrelevant_keywords):\n",
    "        return False\n",
    "    return ingredient.lower() in text_lower\n",
    "\n",
    "def query_openfda(ingredient):\n",
    "    try:\n",
    "        base_url = \"https://api.fda.gov/food/enforcement.json\"\n",
    "        params = {\"search\": f\"product_description:{ingredient}\", \"limit\": 5}\n",
    "        r = requests.get(base_url, params=params).json()\n",
    "        return [rec[\"reason_for_recall\"] for rec in r.get(\"results\", []) if is_fda_entry_relevant(rec[\"reason_for_recall\"], ingredient)]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def query_rxnorm(ingredient):\n",
    "    try:\n",
    "        url = \"https://rxnav.nlm.nih.gov/REST/rxcui.json\"\n",
    "        rxcui = requests.get(url, params={\"name\": ingredient}).json()\n",
    "        return rxcui.get(\"idGroup\", {}).get(\"rxnormId\", [])\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "def query_academic_health_docs(ingredient, max_results=50):\n",
    "    def query_europe_pmc(max_results=50):\n",
    "        try:\n",
    "            base_url = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n",
    "            query = f'\"{ingredient}\" AND (nutrition OR health OR diet)'\n",
    "            params = {\n",
    "                \"query\": query,\n",
    "                \"format\": \"json\",\n",
    "                \"resultType\": \"core\",\n",
    "                \"sort\": \"P_PDATE_D\",\n",
    "                \"pageSize\": max_results\n",
    "            }\n",
    "            r = requests.get(base_url, params=params).json()\n",
    "            results = []\n",
    "            for record in r.get(\"resultList\", {}).get(\"result\", []):\n",
    "                abstract = record.get(\"abstractText\")\n",
    "                if not abstract or len(abstract.strip()) < 100:\n",
    "                    continue\n",
    "                results.append({\n",
    "                    \"title\": record.get(\"title\"),\n",
    "                    \"source\": record.get(\"journalTitle\"),\n",
    "                    \"pubdate\": record.get(\"firstPublicationDate\", record.get(\"pubYear\")),\n",
    "                    \"url\": f\"https://europepmc.org/article/{record.get('source')}/{record.get('id')}\",\n",
    "                    \"full_text\": abstract.strip()\n",
    "                })\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"[EuropePMC JSON API error for '{ingredient}']: {e}\")\n",
    "            return []\n",
    "\n",
    "    def query_pubmed_central(max_results=50):\n",
    "        try:\n",
    "            search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "            params = {\n",
    "                \"db\": \"pmc\",\n",
    "                \"term\": f'\"{ingredient}\" AND (nutrition OR health OR diet)',\n",
    "                \"retmode\": \"json\",\n",
    "                \"retmax\": max_results,\n",
    "                \"sort\": \"pub+date\"\n",
    "            }\n",
    "            r = requests.get(search_url, params=params).json()\n",
    "            ids = r.get(\"esearchresult\", {}).get(\"idlist\", [])\n",
    "            summaries = []\n",
    "            for pmid in ids:\n",
    "                try:\n",
    "                    summary_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi\"\n",
    "                    summary_resp = requests.get(summary_url, params={\"db\": \"pmc\", \"id\": pmid, \"retmode\": \"json\"}).json()\n",
    "                    result = summary_resp.get(\"result\", {}).get(pmid)\n",
    "                    if not result:\n",
    "                        continue\n",
    "                    title = result.get(\"title\", \"\")\n",
    "                    url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC{pmid}/\"\n",
    "                    summaries.append({\"title\": title, \"url\": url, \"full_text\": title})\n",
    "                except:\n",
    "                    continue\n",
    "            return summaries\n",
    "        except Exception as e:\n",
    "            print(f\"[PubMedCentral error for '{ingredient}']: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Combine and deduplicate by title\n",
    "    pmc_results = query_pubmed_central()\n",
    "    europepmc_results = query_europe_pmc()\n",
    "    combined = pmc_results + europepmc_results\n",
    "    seen_titles = set()\n",
    "    unique_results = []\n",
    "    for r in combined:\n",
    "        if r[\"title\"] and r[\"title\"] not in seen_titles:\n",
    "            unique_results.append(r)\n",
    "            seen_titles.add(r[\"title\"])\n",
    "    return unique_results[:max_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NER + Semantic Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text, aliases=None, health_keywords=None):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    invalid_labels = {\n",
    "        \"CARDINAL\", \"DATE\", \"ORDINAL\", \"PERCENT\", \"LANGUAGE\", \"TIME\", \"QUANTITY\", \"MONEY\", \"NORP\", \"EVENT\"\n",
    "    }\n",
    "    forbidden_words = {\n",
    "        \"recall\", \"product\", \"distribution\", \"ingredient\", \"label\", \"cookie\", \"brownie\", \"package\",\n",
    "        \"expiration\", \"sell\", \"pasteurization\"\n",
    "    }\n",
    "\n",
    "    spans = []\n",
    "    for ent in doc.ents:\n",
    "        span_text = ent.text.strip()\n",
    "        span_clean = span_text.lower()\n",
    "        label = ent.label_\n",
    "\n",
    "        # Re-tag known aliases incorrectly labeled as PERSON\n",
    "        if label == \"PERSON\" and aliases and span_clean in aliases:\n",
    "            print(f\"[NER Correction] '{span_text}' was labeled as PERSON, relabeling as INGREDIENT\")\n",
    "            label = \"INGREDIENT\"\n",
    "\n",
    "        # Skip irrelevant\n",
    "        if label in invalid_labels:\n",
    "            continue\n",
    "        if len(span_text) < 3:\n",
    "            continue\n",
    "        if any(word in span_clean for word in forbidden_words):\n",
    "            continue\n",
    "\n",
    "        # Contextual scoring for filtering or ranking\n",
    "        context_window = text[max(0, ent.start_char - 50):ent.end_char + 50].lower()\n",
    "        context_score = sum(1 for kw in (health_keywords or []) if kw in context_window)\n",
    "\n",
    "        spans.append((ent.start_char, ent.end_char, label, span_text, context_score))\n",
    "\n",
    "    # Deduplicate overlapping spans (keep most relevant)\n",
    "    merged = []\n",
    "    spans.sort(key=lambda x: (x[0], -(x[1]-x[0])))\n",
    "    for start, end, label, text_span, score in spans:\n",
    "        if merged and start < merged[-1][1]:\n",
    "            prev = merged[-1]\n",
    "            if score > prev[4]:  # keep if more relevant\n",
    "                merged[-1] = (start, end, label, text_span, score)\n",
    "        else:\n",
    "            merged.append((start, end, label, text_span, score))\n",
    "\n",
    "    return [(text[start:end], label) for start, end, label, _, _ in merged]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scraping and Google CSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web_snippets(ingredient, num_results=50, api_key=None, cse_id=None):\n",
    "    if not api_key or not cse_id:\n",
    "        raise ValueError(\"Google API key and CSE ID are required.\")\n",
    "\n",
    "    if ingredient in search_cache:\n",
    "        return search_cache[ingredient]\n",
    "\n",
    "    #query = f'\"{ingredient}\" AND (nutrition OR dietary OR health OR benefits OR concerns OR restrictions)'\n",
    "    query = f'\"{ingredient}\" AND (health OR diet OR nutrition)'\n",
    "\n",
    "    try:\n",
    "        service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "        res = service.cse().list(q=query, cx=cse_id, num=num_results, sort='date').execute()\n",
    "        items = res.get(\"items\", [])\n",
    "        snippets = [item.get(\"snippet\", \"\") for item in items if item.get(\"snippet\")]\n",
    "        print(f\"[Google CSE Success] Retrieved {len(snippets)} snippets for '{ingredient}'\")\n",
    "        print(f\"[Google CSE Log] Query: {query}\")\n",
    "        for i, snippet in enumerate(snippets[:3]):\n",
    "            print(f\"  Snippet {i+1}: {snippet}\")\n",
    "        search_cache[ingredient] = snippets\n",
    "        return snippets\n",
    "    except Exception as e:\n",
    "        print(f\"[Google Search Error for '{ingredient}']: {e}\")\n",
    "        return []\n",
    "\n",
    "def semantic_scrape_summary(ingredient, api_key=None, cse_id=None):\n",
    "    fallback_attempts = 0\n",
    "    all_snippets = search_web_snippets(ingredient, api_key=api_key, cse_id=cse_id)\n",
    "\n",
    "    if not all_snippets:\n",
    "        fallback_attempts += 1\n",
    "        fallback_term = re.sub(r'[^\\w\\s]', '', ingredient)\n",
    "        print(f\"[Fallback] Trying sanitized alias: '{fallback_term}'\")\n",
    "        all_snippets = search_web_snippets(fallback_term, api_key=api_key, cse_id=cse_id)\n",
    "\n",
    "    if not all_snippets:\n",
    "        print(f\"[Fallback] Google CSE returned no snippets even after fallback. Skipping.\")\n",
    "        return []\n",
    "\n",
    "    all_ents = []\n",
    "    for i, snippet in enumerate(all_snippets):\n",
    "        if not snippet.strip():\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[Google Snippet #{i+1} for '{ingredient}']:\\n{snippet}\")\n",
    "        \"\"\"ents = extract_entities(snippet, aliases=global_alias_set, health_keywords=[\n",
    "            \"health\", \"benefit\", \"risk\", \"effect\", \"toxicity\", \"nutrition\", \"nutrient\", \"disease\", \"metabolism\", \"metabolic\",\n",
    "            \"wellness\", \"digestion\", \"digestive\", \"immunity\", \"immune\", \"safety\", \"intolerance\", \"allergy\", \"reaction\",\n",
    "            \"therapy\", \"treatment\", \"deficiency\", \"excess\", \"overdose\", \"lead to\", \"cause\", \"inflammation\", \"danger\", \"impact\",\n",
    "            \"cardiovascular\", \"result\", \"liver\", \"kidney\", \"blood\", \"hormone\", \"cholesterol\", \"diabetes\", \"body\", \"brain\",\n",
    "            \"mental\", \"physical\", \"absorption\", \"energy\", \"weight\", \"fat\", \"calories\", \"fitness\", \"detox\", \"toxins\",\n",
    "            \"cognitive\", \"focus\", \"clarity\", \"cancer\", \"symptom\", \"pain\", \"relief\", \"anti-inflammatory\", \"sensitivity\",\n",
    "            \"cramps\", \"gut\", \"probiotic\", \"enzyme\", \"alkaline\", \"acidic\", \"bloating\", \"constipation\", \"restriction\", \"avoid\"\n",
    "        ])\"\"\"\n",
    "        ents = extract_entities(snippet, aliases=global_alias_set, health_keywords=keyword_config[\"all\"])\n",
    "        if ents:\n",
    "            print(\"  → Extracted Entities:\")\n",
    "            for text, label in ents:\n",
    "                print(f\"     - {text} ({label})\")\n",
    "        else:\n",
    "            print(\"  → No entities found.\")\n",
    "\n",
    "        all_ents.extend(ents)\n",
    "\n",
    "    return list(set(all_ents))  # Deduplicate final entity list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Processing Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_health_info(ingredient):\n",
    "    return {\n",
    "        \"PubMed\": query_pubmed(ingredient),\n",
    "        \"OpenFDA\": query_openfda(ingredient),\n",
    "        \"RxNorm\": query_rxnorm(ingredient),\n",
    "        \"Academic_Articles\": query_academic_health_docs(ingredient)\n",
    "    }\n",
    "\n",
    "def preprocess_ingredient_list_with_health(text):\n",
    "    raw_ingredients = re.split(r'[\\,\\n;/••]+', text)\n",
    "    processed = []\n",
    "    seen_terms = set()\n",
    "\n",
    "    for raw in raw_ingredients:\n",
    "        raw = raw.strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "\n",
    "        if not is_phonetically_valid(raw):\n",
    "            print(f\"[!] Skipping '{raw}' — unlikely to be a valid ingredient (too short or invalid vowel/consonant pattern)\")\n",
    "            continue\n",
    "\n",
    "        standard, aliases = standardize_ingredient_name(raw)\n",
    "        aliases = list(set([standard] + aliases))\n",
    "        filtered_aliases = [a for a in aliases if is_relevant_alias(a)]\n",
    "        \n",
    "        # Sort and limit to top 5 by similarity\n",
    "        ranked_aliases = sorted(\n",
    "            filtered_aliases,\n",
    "            key=lambda x: fuzz.token_sort_ratio(raw.lower(), x.lower()),\n",
    "            reverse=True\n",
    "        )\n",
    "        selected_aliases = ranked_aliases[:5]\n",
    "\n",
    "        combined_health_info = {\n",
    "            \"PubMed\": [],\n",
    "            \"OpenFDA\": [],\n",
    "            \"RxNorm\": [],\n",
    "            \"NER_Snippets\": [],\n",
    "            \"Academic_Articles\": []\n",
    "        }\n",
    "\n",
    "        found_any_data = False\n",
    "\n",
    "        for term in selected_aliases:\n",
    "            if term.lower() in seen_terms:\n",
    "                continue\n",
    "            seen_terms.add(term.lower())\n",
    "\n",
    "            api_info = get_all_health_info(term)\n",
    "            ner_info = semantic_scrape_summary(term, api_key=GOOGLE_API_KEY, cse_id=GOOGLE_CSE_ID)\n",
    "\n",
    "            if any(api_info[k] for k in api_info) or ner_info:\n",
    "                found_any_data = True\n",
    "\n",
    "            for k in combined_health_info:\n",
    "                if k == \"NER_Snippets\":\n",
    "                    combined_health_info[k].extend([i for i in ner_info if i not in combined_health_info[k]])\n",
    "                else:\n",
    "                    combined_health_info[k].extend([i for i in api_info[k] if i not in combined_health_info[k]])\n",
    "\n",
    "        if not found_any_data:\n",
    "            print(f\"⚠️ No data found for '{raw}'. Did you spell this correctly or is it too obscure?\")\n",
    "\n",
    "        processed.append({\n",
    "            \"standard\": standard,\n",
    "            \"aliases\": aliases,\n",
    "            \"health_info\": combined_health_info\n",
    "        })\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def split_into_token_chunks(text, max_tokens=2000, model=\"gpt-3.5-turbo\"):\n",
    "    doc = nlp_sentencizer(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sent in doc.sents:\n",
    "        if num_tokens(current_chunk + sent.text, model) > max_tokens:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent.text\n",
    "        else:\n",
    "            current_chunk += \" \" + sent.text\n",
    "\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens_per_message = 4  # each message key structure (role, content, etc.)\n",
    "    tokens = 0\n",
    "    for message in messages:\n",
    "        tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            tokens += len(encoding.encode(value))\n",
    "    return tokens + 2  # every reply is primed with <|start|>assistant\n",
    "\n",
    "def classify_chunk(chunk, model=\"your-local-llm\"):\n",
    "    # Placeholder: use your own model here\n",
    "    # This could be a call to a HuggingFace pipeline or custom model\n",
    "    return \"Category: [Benefits]\\nReason: Simulated classification for now.\"\n",
    "\n",
    "def summarize_by_category(classified_chunks, model=\"your-local-llm\"):\n",
    "    # Placeholder: simulate summary for now\n",
    "    summaries = {}\n",
    "    for cat, texts in defaultdict(list).items():\n",
    "        summaries[cat] = \"Simulated summary.\"\n",
    "    return summaries\n",
    "\n",
    "def enrich_with_health_summaries(results, model=\"gpt-3.5-turbo\"):\n",
    "    for entry in results:\n",
    "        all_chunks = []\n",
    "\n",
    "        for article in entry[\"health_info\"].get(\"Academic_Articles\", []):\n",
    "            text = article.get(\"full_text\")\n",
    "            if text and len(text.strip()) > 200:\n",
    "                chunks = split_into_token_chunks(text, max_tokens=2000, model=model)\n",
    "                all_chunks.extend(chunks)\n",
    "\n",
    "        if not all_chunks:\n",
    "            # Try fallback: summarize from available titles + sources\n",
    "            fallback_chunks = []\n",
    "            for article in entry[\"health_info\"].get(\"Academic_Articles\", []):\n",
    "                title = article.get(\"title\", \"\").strip()\n",
    "                source = article.get(\"source\", \"\").strip()\n",
    "                if len(title) > 20:\n",
    "                    snippet = f\"Title: {title}\"\n",
    "                    if source:\n",
    "                        snippet += f\" | Source: {source}\"\n",
    "                    fallback_chunks.append(snippet)\n",
    "\n",
    "            if fallback_chunks:\n",
    "                classified = [(chunk, classify_chunk(chunk, model=model)) for chunk in fallback_chunks]\n",
    "                summaries = summarize_by_category(classified, model=model)\n",
    "                entry[\"health_summary\"] = summaries\n",
    "            else:\n",
    "                entry[\"health_summary\"] = {\"note\": \"No usable full-text content or fallback title.\"}\n",
    "            continue\n",
    "\n",
    "        # Regular summarization path\n",
    "        classified = [(chunk, classify_chunk(chunk, model=model)) for chunk in all_chunks]\n",
    "        summaries = summarize_by_category(classified, model=model)\n",
    "        entry[\"health_summary\"] = summaries\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### llama3gptq integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- New Summarization Functions ---\n",
    "def generate_summary(text, category, ingredient_name, model, tokenizer):\n",
    "    if category == \"benefits\":\n",
    "        instruction = f\"From the following scientific findings, summarize how {ingredient_name} affects human health. Only include specific beneficial effects backed by studies. Limit to 5 bullet points. Format each point as a separate line starting with a dash.\"\n",
    "    elif category == \"concerns\":\n",
    "        instruction = f\"From the following scientific findings, summarize how {ingredient_name} affects human health. Only include specific negative effects backed by studies. Limit to 5 bullet points. Format each point as a separate line starting with a dash.\"\n",
    "    elif category == \"restrictions\":\n",
    "        instruction = f\"From the following scientific findings, summarize how {ingredient_name} affects human dietary restrictions (such as allergies, intolerances, and religious restrictions). Limit to 5 bullet points. Format each point as a separate line starting with a dash.\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid category\")\n",
    "\n",
    "    prompt = instruction + \"\\n\\n\" + text\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract bullet lines\n",
    "    bullets = [line.strip() for line in summary.splitlines() if line.strip().startswith(\"-\")]\n",
    "\n",
    "    # Deduplicate semantically\n",
    "    unique_bullets = []\n",
    "    for bullet in bullets:\n",
    "        if all(fuzz.ratio(bullet, b) < 90 for b in unique_bullets):\n",
    "            unique_bullets.append(bullet)\n",
    "\n",
    "    return \"\\n\".join(unique_bullets[:5]) if unique_bullets else summary\n",
    "\n",
    "def clean_and_chunk_sentences(text, health_keywords=None, max_sentences=20):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    \n",
    "    if health_keywords:\n",
    "        sentences = [s for s in sentences if any(kw in s.lower() for kw in health_keywords)]\n",
    "    \n",
    "    # Chunk sentences into groups\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), max_sentences):\n",
    "        chunks.append(\" \".join(sentences[i:i+max_sentences]))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def deduplicate_sentences(sentences, threshold=92):\n",
    "    \"\"\"Deduplicate semantically similar sentences based on fuzzy matching.\"\"\"\n",
    "    unique = []\n",
    "    for s in sentences:\n",
    "        if all(fuzz.ratio(s, u) < threshold for u in unique):\n",
    "            unique.append(s)\n",
    "    return unique\n",
    "\n",
    "def truncate_to_token_limit(text, tokenizer, max_tokens=800):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_tokens)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def filter_sentences(sentences, task):\n",
    "    if task in keyword_config:\n",
    "        keywords = keyword_config[task]\n",
    "        return [s for s in sentences if any(word in s.lower() for word in keywords)]\n",
    "    return sentences\n",
    "\n",
    "def is_probably_human_study(title, abstract):\n",
    "    non_human_terms = [\n",
    "        \"mouse\", \"mice\", \"rat\", \"rabbits\", \"pigs\", \"cattle\", \"chicken\", \"fish\", \"gobies\", \n",
    "        \"canine\", \"murine\", \"insect\", \"drosophila\", \"dog\", \"animal model\", \"rodent\", \"zebrafish\"\n",
    "    ]\n",
    "    combined_text = (title + \" \" + abstract).lower()\n",
    "    return not any(term in combined_text for term in non_human_terms)\n",
    "\n",
    "def extract_relevant_sentences(text, aliases=None):\n",
    "    outcome_terms = [\n",
    "        \"result\", \"conclusion\", \"finding\", \"found\", \"significant\", \"associated\",\n",
    "        \"led to\", \"revealed\", \"observed\", \"showed\", \"demonstrated\", \"lead to\"\n",
    "    ]\n",
    "\n",
    "    skip_phrases = {\"this study aims\", \"background\", \"introduction\", \"study design\", \"was conducted\", \"was performed\"}\n",
    "    keywords = set(keyword_config[\"all\"] + outcome_terms)\n",
    "    aliases = [a.lower() for a in (aliases or [])]\n",
    "\n",
    "    return [\n",
    "        sent.text for sent in nlp(text).sents\n",
    "        if (\n",
    "            any(kw in sent.text.lower() for kw in keywords)\n",
    "            and any(alias in sent.text.lower() for alias in aliases)\n",
    "            and not any(phrase in sent.text.lower() for phrase in skip_phrases)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "def enrich_with_health_summaries_v2(results, model, tokenizer):\n",
    "    health_keywords = keyword_config[\"all\"]\n",
    "\n",
    "    for entry in results:\n",
    "        ingredient_name = entry['standard']\n",
    "        articles = entry[\"health_info\"].get(\"Academic_Articles\", [])\n",
    "        print(f\"\\n🧪 Processing ingredient: {ingredient_name}\")\n",
    "        print(f\"→ Total academic articles found: {len(articles)}\")\n",
    "\n",
    "        if not articles:\n",
    "            entry[\"health_summary\"] = {\"note\": \"No academic articles available.\"}\n",
    "            continue\n",
    "\n",
    "        human_articles = [\n",
    "            a for a in articles \n",
    "            if is_probably_human_study(a.get(\"title\", \"\"), a.get(\"full_text\", \"\"))\n",
    "        ]\n",
    "        print(f\"→ Human-relevant articles retained: {len(human_articles)}\")\n",
    "        print(f\"→ Articles rejected for non-human focus: {len(articles) - len(human_articles)}\")\n",
    "\n",
    "        if not human_articles:\n",
    "            entry[\"health_summary\"] = {\"note\": \"No human-relevant articles found.\"}\n",
    "            continue\n",
    "\n",
    "        entry[\"health_summary\"] = {}\n",
    "\n",
    "        for category in [\"benefits\", \"concerns\", \"restrictions\"]:\n",
    "            category_summaries = []\n",
    "            total_relevant_sentences = 0\n",
    "\n",
    "            for article in human_articles:\n",
    "                raw_text = article.get(\"full_text\", \"\")\n",
    "                if not raw_text:\n",
    "                    continue\n",
    "\n",
    "                relevant_sentences = extract_relevant_sentences(raw_text, aliases=entry.get(\"aliases\", []))\n",
    "                total_relevant_sentences += len(relevant_sentences)\n",
    "\n",
    "                filtered_sentences = filter_sentences(relevant_sentences, category)\n",
    "                filtered_sentences = deduplicate_sentences(filtered_sentences)\n",
    "\n",
    "                if not filtered_sentences:\n",
    "                    continue\n",
    "\n",
    "                # Truncate cleanly by tokens\n",
    "                combined_text = \" \".join(filtered_sentences)\n",
    "                truncated_input = truncate_to_token_limit(combined_text, tokenizer)\n",
    "\n",
    "                summary = generate_summary(\n",
    "                    truncated_input, category, ingredient_name, model, tokenizer\n",
    "                )\n",
    "                category_summaries.append(summary)\n",
    "\n",
    "            print(f\"→ [{category.upper()}] Relevant sentences across all articles: {total_relevant_sentences}\")\n",
    "            print(f\"→ [{category.upper()}] Summaries generated: {len(category_summaries)}\")\n",
    "\n",
    "            if category_summaries:\n",
    "                entry[\"health_summary\"][category] = \"\\n\".join(list(set(category_summaries)))\n",
    "            else:\n",
    "                entry[\"health_summary\"][category] = f\"No relevant {category} information found.\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample4 = \"whey protein isolate, sugar, edamame\"\n",
    "results4 = preprocess_ingredient_list_with_health(sample4)\n",
    "\n",
    "# Run LLM-based summarization on your enriched ingredient data\n",
    "results4 = enrich_with_health_summaries_v2(results4, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results cleanly\n",
    "for entry in results4:\n",
    "    print(f\"\\n🧪 Ingredient: {entry['standard']}\")\n",
    "    print(\"→ Aliases:\", entry[\"aliases\"])\n",
    "\n",
    "    print(\"\\nTrusted API Info:\")\n",
    "    for source, data in entry[\"health_info\"].items():\n",
    "        if source == \"NER_Snippets\":\n",
    "            continue\n",
    "        print(f\"  • {source}:\")\n",
    "        if isinstance(data, list) and data:\n",
    "            for item in data:\n",
    "                if isinstance(item, str):\n",
    "                    print(f\"     - {item}\")\n",
    "                elif isinstance(item, dict):\n",
    "                    print(f\"     - {item.get('title', '')}\")\n",
    "        elif isinstance(data, list):\n",
    "            print(\"     - No results\")\n",
    "        else:\n",
    "            print(f\"     - {data}\")\n",
    "\n",
    "    print(\"\\nNER Entities from Web Snippets:\")\n",
    "    if entry[\"health_info\"][\"NER_Snippets\"]:\n",
    "        for ent_text, ent_label in entry[\"health_info\"][\"NER_Snippets\"]:\n",
    "            print(f\"     - {ent_text} ({ent_label})\")\n",
    "    else:\n",
    "        print(\"     - No named entities found.\")\n",
    "\n",
    "    print(\"\\n💬 LLM-Generated Health Summaries:\")\n",
    "    for category, summary in entry.get(\"health_summary\", {}).items():\n",
    "        print(f\"  [{category.upper()}]: {summary}\")\n",
    "\n",
    "save_search_cache()\n",
    "save_alias_cache()\n",
    "save_alias_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LLM Improvements TO-DO:\n",
    "- Removing duplicates\n",
    "- Combining into one summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADD EVALUATION METRICS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3gptq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
